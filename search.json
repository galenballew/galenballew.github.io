[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "The Mythical Man-Month\n\n\n\n\n\nTL;DR - Looking at managing large software projects from 1975 to 1995.\n\n\n\n\n\nJan 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nKubernetes Best Practices\n\n\n\n\n\n\ncloud\n\n\nk8s\n\n\ntldr\n\n\nkubernetes\n\n\n\nTL;DR - Blueprints for building successful applications on Kubernetes.\n\n\n\n\n\nSep 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Win Friends & Influence People\n\n\n\n\n\n\ntldr\n\n\n\nTL;DR - because people ain’t easy.\n\n\n\n\n\nSep 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTerraform for Dummies\n\n\n\n\n\n\ncloud\n\n\ndevops\n\n\ncertification\n\n\nhashicorp\n\n\nterraform\n\n\n\nPreparing for (and passing) the Terraform Associate Certification.\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDigital Gardens\n\n\n\n\n\n\nknowledge management\n\n\nblogging\n\n\n\nThoughts and inspiration about building your digital garden for knowledge management and networked thought.\n\n\n\n\n\nJul 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR - Kubernetes Up & Running, Part 4\n\n\n\n\n\n\ngcp\n\n\ncloud\n\n\nk8s\n\n\naws\n\n\nkubernetes\n\n\n\nThe saga concludes.\n\n\n\n\n\nSep 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR - Kubernetes Up & Running, Part 3\n\n\n\n\n\n\ngcp\n\n\ncloud\n\n\nk8s\n\n\naws\n\n\nkubernetes\n\n\n\nHow deep it goes, no one knows.\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR - Kubernetes Up & Running, Part 2\n\n\n\n\n\n\ngcp\n\n\ncloud\n\n\nk8s\n\n\naws\n\n\nkubernetes\n\n\n\nDive deeper into the future of infrastructure.\n\n\n\n\n\nSep 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTL;DR - Kubernetes Up & Running, Part 1\n\n\n\n\n\n\ngcp\n\n\ncloud\n\n\nk8s\n\n\naws\n\n\nkubernetes\n\n\n\nDive into the future of infrastructure.\n\n\n\n\n\nSep 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWhy and How Captialism Needs to be Reformed by Ray Dalio\n\n\n\n\n\n\ntldr\n\n\nfinance\n\n\n\n\n\n\n\n\n\nAug 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nGCP Professional Cloud Developer Certification\n\n\n\n\n\n\ngcp\n\n\ncertification\n\n\ndevops\n\n\n\nBecome the AppDev wizard you were meant to be.\n\n\n\n\n\nAug 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAWS Solutions Architect Professional Study Guide\n\n\n\n\n\n\naws\n\n\ncloud\n\n\ncertification\n\n\n\nA study guide based on the new 2019 exam guide.\n\n\n\n\n\nApr 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nThe 6 Rs of Cloud Migration\n\n\n\n\n\n\ntldr\n\n\ncloud\n\n\naws\n\n\n\nTL;DR on this AWS blog post about 6 migration strategies.\n\n\n\n\n\nApr 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Built This Website\n\n\n\n\n\n\naws\n\n\nblogging\n\n\n\nThis post details how I setup the website you’re looking at! It includes topics like domain registration, DNS, AWS S3 static website hosting, and setting up a CICD pipeline.\n\n\n\n\n\nFeb 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Landed My Dream Job Working On Self-Driving Cars\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\nbootcamps\n\n\nself-driving cars\n\n\n\nFrom a BS in Mathematics to working on bleeding-edge technology\n\n\n\n\n\nMay 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nAre Tech Bootcamps Worth It?\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\nbootcamps\n\n\n\nMy 12 weeks of data science at Metis\n\n\n\n\n\nMay 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nOpenCV for Lane Detection in Self Driving Cars\n\n\n\n\n\n\ndeep learning\n\n\nself-driving cars\n\n\nmachine learning\n\n\n\nBuilding a lane detection system using Python 3 and OpenCV\n\n\n\n\n\nFeb 17, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBoard Games Meet Machine Learning\n\n\n\n\n\n\ndata science\n\n\nmachine learning\n\n\n\nUsing scikit-learn to analyze board game data using linear regression\n\n\n\n\n\nFeb 3, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Visualization of S3\n\n\n\n\n\n\nmathematics\n\n\nunity\n\n\nvr\n\n\nvisualization\n\n\n\nStereographic projection of a 4-dimensional dataset into 3d virtual reality\n\n\n\n\n\nAug 1, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nElliptic Curves\n\n\n\n\n\n\nmathematics\n\n\n3d printing\n\n\nautodesk maya\n\n\n\nElliptic curves with complex multiplication give rise to tori over the complex numbers\n\n\n\n\n\nMay 25, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSciVis 2016\n\n\n\n\n\n\nvisualization\n\n\n\nCavern Halos: Dark Sky Project visualization\n\n\n\n\n\nApr 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nMetaselfie\n\n\n\n\n\n\nvisualization\n\n\nautodesk maya\n\n\nmathematics\n\n\n\nFractal animation with original soundscape\n\n\n\n\n\nDec 5, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nVisAP 2015\n\n\n\n\n\n\nvisualization\n\n\n\nData Improvisations: The IEEE Visual Arts Program Exhibition\n\n\n\n\n\nOct 27, 2015\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive Fractal Generator\n\n\n\n\n\n\nmathematics\n\n\nunity\n\n\nvisualization\n\n\n\nInteractive Unity3d build of a box fractal\n\n\n\n\n\nApr 11, 2015\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-04-11-scivis/2016-04-11-scivis.html",
    "href": "posts/2016-04-11-scivis/2016-04-11-scivis.html",
    "title": "SciVis 2016",
    "section": "",
    "text": "All video and editing done by Galen Ballew."
  },
  {
    "objectID": "posts/2017-02-03-board-games-machine-learning/2017-02-03-board-games-machine-learning.html",
    "href": "posts/2017-02-03-board-games-machine-learning/2017-02-03-board-games-machine-learning.html",
    "title": "Board Games Meet Machine Learning",
    "section": "",
    "text": "I have this article published over on Medium. Click here to read it."
  },
  {
    "objectID": "posts/2020-09-16-k8s-best-practices/2020-09-16-k8s-best-practices.html",
    "href": "posts/2020-09-16-k8s-best-practices/2020-09-16-k8s-best-practices.html",
    "title": "Kubernetes Best Practices",
    "section": "",
    "text": "Table of Contents\n\nAbstract\nSetting Up a Basic Service\nDeveloper Workflows\nMonitoring and Logging in Kubernetes\nConfiguration, Secrets, and RBAC\nContinuous Integration, Testing, and Deployment\nVersioning, Releases, and Rollouts\nWorldwide Application Distribution and Staging\nResource Management\nNetworking, Network Security, and Service Mesh\n\n\n\nAbstract\nKubernetes Best Practices was authored by Brendan Burns, Eddie Villalba, Dave Strebel, and Lachlan Evenson. These are the working notes that I took while reading the book. I have a solid working knowledge of Kubernetes and did not seek to regurgitate everything I read. Instead, I hope that this article consists of the juiciest tid bits and highlights that are easily consumable for anyone familiar with k8s. Enjoy!\n\n\nSetting Up a Basic Service\nKubernetes starts with containers - immutable containers to be specific. Do not use the latest tag for your image versions. Instead, use a combination of semantic versioning and the SHA hash of the commit where the image was built (e.g., v1.0.1-bfeda01f).\nIt’s recommended to set a container’s Requests and Limits are set equal to each other. This results in very predictable pod behavior, as well as predictable utilization. You can certainly set the values for Requests and Limits independently in order to drive maximal utilization, but most users find that the stability/predictability from setting them equal is more valuable than the gained utilization.\nThe key/values in a ConfigMap are subject to change. When a key/value needs to be updated, it may be tempting to edit the ConfigMap YAML and apply the update in-place. However, this will not trigger an update to existing Pods using the ConfigMap - they will only use the new configuration after a restart. Rather than updating the existing ConfigMap, adding version numbers to your ConfigMaps, deploying an entirely new ConfigMap, and updating the Deployment to use this new one. Rollout of the new configuration will be automatically triggered. Additionally, the previous version of the ConfigMap will still be available in the cluster - rollback is just a matter of updating the Deployment again.\nSecrets are stored unencrypted within etcd. If you want to protect that sensitive data from a direct attack against etcd, you can provide Kubernetes with a key that it will use to encrypt the data at rest. There’s more information available in the documentation.\nGenerally speaking, managing state is hard. Also, generally speaking, stateful services (e.g., Redis as a service) are worth the extra cost. The book doesn’t offer any additional information or opinions beyond this, but there is an excellent blog article by Google about mapping to external services. The book has an entire chapter on integrating external services, as well as a chapter on managing state and stateful applications!\nEven if you’re an individual developer, you will eventually want your k8s configuration yaml to be deployed to multiple, different endpoints (e.g., dev, stage, prod). Rather than having multiple copies of the same code, you should use a packing tool like Helm to parametrize your configuration. Instead of having multiple copies of application YAML, you can have a single values.yaml per deployment environment and you can keep them all in a templates/ directory at the root of your project. The book doesn’t talk about it, but this also gives you the added benefit of building modular, reusable k8s building blocks. These can be shared and leveraged across the company so that teams don’t need to reinvent the wheel.\n\n\nDeveloper Workflows\nWhen thinking about developer interactions with a development cluster, it can be useful to break it down into three phases: 1. Onboarding - getting access to the cluster 2. Developing - getting bootstrapped and able to deploy 3. Testing - being able to iterate quickly and efficiently\nGenerally speaking, it’s a best practice to use a single, large cluster for all developers and break it down into namespaces to keep things tidy. There is some additional overhead of managing this cluster as opposed to creating individual clusters per developer, but the increased resource utilization and availability of shared services (e.g., logging) make it worthwhile. When you’re operating in this model, it is also a good idea to give everyone read access to the entire cluster. This can help everyone debug in case your neighbor is breaking your deployment or hogging all the resources. However, it’s worth noting that the default read access will include Secrets. You can create fine-grain permissions that exclude Secrets if necessary, but it’s usually not a problem in a development environment.\nIt can be extremely useful to automate as much of the namespace management as possible. This could be a script that creates a new namespace, adds users to it, defines the Requests and Limits, and sets a TTL. The idea of a TTL is useful because it helps keep the cluster lean and clean (and helps build good developer habits). You can enforce the TTL by writing a CronJob to clean them up. You could also replace the script that instantiates the namespace with a CRD!\nIf your development namespace has a TTL, then you’re definitely going to want a script that installs all the necessary dependencies for your project. It is also a best practice to have a script that will delete and recreate your Deployments rather than update them in-place. In a development environment, you want your code changes to rollout quickly, but modifying the k8s rollout logic should only be done with extreme caution - you do not want to create drift between your development and production clusters.\n\n\nMonitoring and Logging in Kubernetes\nThere are two monitoring paradigms that complement each other and are useful in the context of Kubernetes: USE and RED.\n\nU - Utilization\nS - Saturation\nE - Errors\n\n\nR - Rate\nE - Errors\nD - Duration\n\nUSE focuses more on the infrastructure, whereas RED focuses on the end-user experience for the application. These patterns should help determine what you monitor, because monitoring everything is counter-productive (poor signal:noise ratio).\nRather than focusing on the details of the Metrics API, Metrics Server, kube-state-metrics, or any of the multitude of monitoring tools available (e.g., Prometheus), I’d like to focus on what metrics should be captured. Monitoring should take a layered approach that takes into account the following: - Physical or virtual nodes - Cluster components - Cluster add-ons - End-user applications\nGiven these layers, what metrics should be targeted? - Nodes - CPU utilization - Memory utilization - Network utilization - Disk utilization - Cluster components - etcd latency - Cluster add-ons - Cluster Autoscaler - Ingress Controller - Application - Container memory utilization and saturation - Container CPU utilization - Container network utilization and error rate - Application frame-work specific metrics\nWith regards to logging, there are several components that you will need to capture logs from: - Node logs - Docker daemon - Kubernetes control-plane logs - API server - Controller manager - Scheduler - Kubernetes audit logs - Application controller logs\nIt is worth mentioning that k8s audit logs can be extremely noisy. It’s worth using the documentation to fine-tune them for your environment.\nLimit the usage of log forwarders in a sidecar pattern - they consume a lot more resources. Opt for using a DaemonSet for the log forwarder and sending the logs to STDOUT.\nThe book mentions SRE practices like defining SLOs and measuring them using SLIs, but those methodologies are beyond the scope of the book. They’re still best practices though and should be implemented in order to provide a highly available, secure Kubernetes platform. All of the Google books on SRE are available for free.\n\n\nConfiguration, Secrets, and RBAC\nGenerally speaking, the best way to approach any changes to a ConfigMap or Secret is to update the entire Deployment. This is because a change to the configuration object will not trigger an update of the Pods that reference/mount it. It is a best practice to use a version number in the name of the configuration object and change the Deployment to point at the new version. This will cause all of the Pods to update. It also makes rollback easy.\nYou can assign an imagePullSecrets to a serviceaccount that the pod will use to automatically mount the secret without having to declare it in the pod.spec. You can do this to the default service account for the namespace and the secret will be automatically added to all the pods in that namespace.\nRBAC has a lot in common with AWS IAM as far as best practices for scope of privilege (TL;DR - use least privilege). I’ve left out most of the information about RBAC, but it is worth explicitly stating that most application that run on Kubernetes will not need an RBAC role or role binding. This is because most applications do not directly interact with the k8s API itself. In the case where an application does need to interface with the k8s API, it should use a purpose-specific service account with a least privileged role to execute its goal.\n\n\nContinuous Integration, Testing, and Deployment\nCI/CD, DevOps, GitOps, etc., all deserve and have their own books about them. With regards to best practices for Kubernetes, there will a few specific and salient points that jumped out at me:\n\nWhatever CI/CD tools that you choose must be able to define the pipeline as code. Storing the pipeline in version control alongside the application code is invaluable.\nOptimize your container images for size. Smaller images means more efficient development and lower security risk.\n\nAs you mature, you may become comfortable using a distroless base image.\nUse multi-stage builds to slim down containers. A classic example is to build your Go static binary and place only that into the production container - not all the packages that were used to build it.\n\nDo not use the latest tag! Every container image tag should point to a specific version/build of the code.\nKubernetes offers several different deployment strategies. If you’re just getting started, rolling deployments are the easiest to use. When using blue/green or canary, be sure to understand the implications for having two versions of your application available - especially as it relates to state and/or hybrid applications.\n\n\n\nVersioning, Releases, and Rollouts\nVersioning should be a relatively straightforward task, but it’s a critical one. There are so many different types of Kubernetes API objects, each with their own version, that making sure everything is properly versioned becomes mission critical in order to enable smooth operations.\nAs far as rollouts are concerned, it’s imperative to understand that changes to the metadata fields of a Deployment will not trigger an update. Only changes to the spec.template will trigger an update. This becomes important as you will have versions for containers, Pods, Deployments, Services, and the application itself - all of which should be distinct from each other.\nUse a release and release version/number in your Deployment metadata. The release name and number should coordinate with the actual release from your CI/CD tooling in order to enable traceability.\nIf you’re using Helm, be sure to bundle services that need to be rolled back or upgraded together. You can use Helm chart hooks to make sure that release lifecycle events go smoothly. Chart hooks allow events (e.g., run a Job, mount a ConfigMap, etc.) to be triggered at specific times in the release lifecycle (e.g., pre-install, post-rollback, etc.)\n\n\n Worldwide Application Distribution and Staging\nStarting with the fundamentals, it’s imperative to have your container images distributed to each region that your workload is running in. This helps to make rollouts more reliable by avoiding any networking issues that may crop up.\nGlobal rollouts should be preceded by rigorous integration testing. Ideally, you will have a copy of your production data available for this - try to think ahead before you’re scaled up. Setting up good integration testing is easier early on in the development of an application and it pays serious dividends in the long run. Load-testing should also be performed. Replaying the logged traffic is a good starting point, but not always fool-proof.\nOnce you’re ready to begin the rollout, a canary region should be selected and deployed to. Your customers will use this region as a preproduction environment to validate their use of your service before continuing the rollout. Generally speaking, a decent rule of thumb is to leave this deployment in the canary region for double the average time-to-smoke.\nLastly, be sure to document and practice your response to any problems or processes that you encounter. There should be runbooks for everything you need to do - nothing should be done from memory.\n\n\nResource Management\nBoth the general guidelines and the specific implementations of resource management depend on the workload and the underlying hardware it requires. Any specialized hardware (e.g., GPUs) should be accessed with taints and/or nodeSelectors. Use nodeSelectors when you want to request specialized hardware and use taints when you want to reserve it. If you’re running multiple workloads with different performance requirements, be sure to include a mix of node pools with different instance types. If those workloads are variable or have unexpected spikes in usage, consider using the Horizontal Pod Autoscaler.\nAs far as Pods go, everything should have its Requests and Limits defined. If a Pod’s Requests and Limits are equal, the Pod will receive a guaranteed Quality of Service (QoS) class. This is partly why it’s recommended to set them equal when you’re getting started on your k8s journey. However, guaranteed QoS requires that Requests and Limits are set for all containers in a Pod.\nAdditionally, use PodDisruptionBudgets to manage how many Pods are (un)available during a disruption. This, in congruence with (anti-)affinity attributes, can ensure that your service remains highly available through out operations and outages.\n\n\nNetworking, Network Security, and Service Mesh\nProbably the single most important decision around networking is what Container Network Interface to use. This deserves its own article. The chosen CNI should deliver the feature set required, be compatible with your control plane (particularly important when consuming a managed service like AWS EKS), and be compatible with your network observability, management, and security tooling. And, if you choose to use a CNI that does not over a network overlay, you’ll need to make sure that you have enough CIDR to handle node IPs, Pod IPs, load balancers, and wiggle room for rollouts and scaling.\nThe next big decision to make is the choice of Ingress controller. Just like choosing a CNI, a thorough investiation of feature set and compatability needs to be performed. Standardize the chosen Ingress controller across the enterpise because many of the specific configuration annotations vary between implementations. If you aren’t consistent, your workloads won’t be portable."
  },
  {
    "objectID": "posts/2021-01-18-mythical-man-month/2021-01-18-mythical-man-month.html",
    "href": "posts/2021-01-18-mythical-man-month/2021-01-18-mythical-man-month.html",
    "title": "The Mythical Man-Month",
    "section": "",
    "text": "Abstract\nSummary\n\nChapter 1. The Tar Pit\nChapter 2. The Mythical Man-Month\nChapter 3. The Surgical Team\nChapter 4. Aristocracy, Democracy, and System Design\nChapter 5. The Second-System Effect\nChapter 6. Passing the World\nChapter 7. Why Did the Tower of Babel Fail?\nChapter 8. Calling the Shot\nChapter 9. Ten Pounds in a Five-Pound Sack\nChapter 10. The Documentary Hypothesis\nChapter 11. Plan to Throw One Away\nChapter 12. Sharp Tools\nChapter 13. The Whole and the Parts\nChapter 14. Hatching a Catastrophe\nChapter 15. The Other Face\nOriginal Epilogue\n\nNo Silver Bullet"
  },
  {
    "objectID": "posts/2021-01-18-mythical-man-month/2021-01-18-mythical-man-month.html#footnotes",
    "href": "posts/2021-01-18-mythical-man-month/2021-01-18-mythical-man-month.html#footnotes",
    "title": "The Mythical Man-Month",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe full quote is \"A team of two, with one leader, is often the best use of minds. [Note God's plan for marriage.]↩︎\nNote the similarities between Brooks advice and that of Dale Carnegie’s How to Win Friends and Influence People↩︎\nThis is a fantastic idea that I aim to adopt in my own architecture practice. I think that formalizing it into a weekly summary produces greater collective grokking than simply updating the necessary wiki pages.↩︎\nIt is remarkable to see that support for practices similar to continuous integration go all the way back to 1975. It is equally remarkable that DevOps only came into its own right around 2012.↩︎"
  },
  {
    "objectID": "posts/2020-07-17-digital-gardens/2020-07-17-digital-gardens.html",
    "href": "posts/2020-07-17-digital-gardens/2020-07-17-digital-gardens.html",
    "title": "Digital Gardens",
    "section": "",
    "text": "I found out about Roam Research a few weeks back (on Hacker News, I think.) I fell into its rabbit hole for a little bit and immediately recognized that I had been seeking for a better way to express and connect my thoughts. I have been using Evernote for the longest time. It gets the job done. Just so long as the job is “archive this thought somewhere so that I don’t forget it forever”. I don’t think that Evernote, as a tool, has actually helped to increase the depth, breadth, or richness of my thoughts.\n\nRoaming Your Thoughts\n\nroam\n/rōm/\nverb\nMove about or travel aimlessly or unsystematically, especially over a wide area.\n\nThe ability to traverse your notes as if they were your second brain requires the notes themselves to provide the pathways and networks to reach other notes. If I am reading my TODO list and I see that I need to shop for groceries, wouldn’t it be nice for the note itself to contain a link to my Shopping List? And when I do look at my Shopping List and I see that I need flour and yeast, wouldn’t it be nice to have a link to my Recipes to view the no-knead, cold fermented bread recipe I’ve been meaning to bake? This is a very simple and practical example - most people won’t forget that chain of causation and relationships between the notes and their content. But imagine a network with hundreds or thousands of Notes, each with multiple [Links] pointing to different Notes. By having the notes themselves contain the pathways to other thoughts/notes, we can build a network that makes it easier to explore, visualize, and create relationships between certain ideas. Our brains do this inherintely, but we can augment this ability with bi-directional linking.\nHere’s how bi-directional linking works in the contextual nutshell of note-taking:\n\nYou are writing a note in Markdown and you make a wiki link to [[my awesome page]].\nIf a note for [[my awesome page]] already exists, [[my awesome page]] will be automagically updated at the bottom with something that says “Pages that link to my awesome page: [[original note]]\nIf note for [[my awesome page]] does not exist, then it will be created and have same link infoformation applied automagically.\n\nAnd thus, each page contains information about all of the pages that it links to and all of the pages that link to it. By continuing to create notes (nodes) and link them to others (edges) we build a graph that we can traverse and visualize - huzzah! This is basically a note-based version of one of my all-time favorite pieces of software, Inspiration 9.\n\nBi-Directional Note Applications\n\nRoam Research\n\nSuper trendy right now, especially in academia\nSaaS\n\nObsidian\n\nSame idea, but runs locally and you can plug into git\n\nFoam\n\nSame-same idea, but does it all in VS Code and git\n\n\n\n\n\nDigital Gardens\nMy immediate thought with graph-based notes was that I’d like to be able to shove the entire network into my blog. It should be no surprise that there are plenty of brilliant people who are way ahead of me 😄 Not only are there excellent blogs that use the underlying technology, but there is an entire philosophy and subculture behind “digital gardening”.\nI stumbled into digital gardening through the backdoor. I started with the technology because it was a natural (if rudimentary) augmentation of how we already think and process thoughts - aside from neural networks, I’m not familiar with other examples of biomimicry in coding/technology. I imagine that the front door to the garden starts with a desire to grow things and produce better ideas. I’ll leave it as an exercise for the reader to figure out what it’s all about, but just remember that it’s Joe Garden; it’s Joe Dirt.\n\nWhat Are You Growing?\n\nYou and your mind garden\n\nAn essay about digital gardening.\n\nMaggie Appleton\n\nA crisp, clean, modern digital garden.\n\nAndy Matuschak’s Working Notes\n\nOne of the most fun and rewarding UI experiences for rambling through notes.\nThese notes are Excalibur-tier legend amongst digital gardeners from what I can gather.\n\nI believe that Andy let a few people use his framework and it has spawned at least one Gatsby theme since then.\n\n\nMaxime Vaillancourt\n\nA blend of traditional blogging and digital gardening - check out the Notes section to see what’s growing.\n\nMaxime’s site includes a graph visualization.\n\nMaxime also has a Side Project for a digital garden jekyll template.\n\n\n\n\n\nAgriculture is the Mother of Invention\nHere is a fun story: While trying to write this article, I had a full-blown digital gardening experience. What you’re about to read is a short story about a few minutes of research that would make a perfect snippet or micro-blog (AKA a “seedling”.)\nI was looking for a subtitle for the next section of my blog article where I talked about what components are missing from the tech stack for a really comprehensive, smooth blogging experience that would use graph-based notes. In the back of my mind, I thought I remembered some quote about “War is the mother of all invention.” and a vague recollection of “..and that’s where microwaves come from, kids!”\nSo I went with “Agrilculture is the Mother of Invention.” because, ya know, puns. I did a quick Google just to cover my bases and I found out that “Necessity is the mother of invention.” But this is where it get’s good - Ester Boserup used the phrase as a major argument in her book The Conditions of Agricultural Growth: The Economics of Agrarian Change under Population Pressure. So, this just goes to show you that my subtitle for this section is about 9,000 IQ.\nI’m a big fan of quotes, so I would probably tag the above snippet with that label and link it to/from this article. It would be so much better if I had actually done that instead of talking about it hypothetically, but that brings me to my next point: there are some tools missing from the toolkit.\nThe key magic behind graph-based note applications is that they automagically create a new note for you when you type out a link. The note exists and it even contains some text; the backlink to the note that originally linked to it. However, we need a little more than that. If I want my new note to appear as an article of sorts on my blog with minimal effort, I need a little more boilerplate text added than just the title of the note and a backlink. At minimum, we’d need to add the Markdown header (if you’re using a Markdown based website.) Here is an example of the Markdown header for this article:\n---\ntitle: 'Digital Gardens'\ndate: 2020-07-17 00:00:00\nexcerpt: Can you dig it?\ncategories: [lifestyle]\ncategories: []\nfeatured_image: 'images/demo/demo-portrait.jpg'\n#scroll_image:\ncomments: true\nshare: true\n---\nWhen a new note is generated by creating a [[link]], that note should come with come with some additional boilerplate besides the backlink - namely a Markdown header for a static site. Once that’s squared away, it would be fantastic to have a lightweight editor that worked on mobile.\nBetween these two additions, I think it would be a very fluid experience to seed, water, and tend your digital garden. Some additional workflows could be added to tag, group, and aggregate micro-blogs, but such is the life of a gardener. A solid website UI could make your blog just as useful to you as anyone else. What I mean by that is the end-user experience of roaming on the blog should be as good or better than in the networked note-taking app itself.\nThere’s a few things out there that I’m keeping my eye on to pull this idea together:\n\nObsidian - integration with Github Pages\nObsidian - notes templates\n\n\n\nFarmer’s Market\nLet’s take this one step further and see if there are any commercial enterprise applications. In my current line of work, I can see an immediate use case for the ability to create a chain of bidirectional links. Here’s a concrete example:\n\nSecurity Architecture defines requirements for a new service.\n\n[[Service Requirements]]\n\nDevOps team builds creates a set of tests according to the architecture. The wiki and/or README for the tests include a backlink to [[Service Requirements]]\n\n[[Service Requirements]] now contains links to the codebase where the requirements are implemented.\n\n\nThis chain could extend laterally as well, aggregating different services and/or platforms. The ability to automatically create a summary of related artifacts without having to manage the links manually is certainly something that I personally would love in my working life.\n\n\nResources\n\nBuilding a Second Brain\nZettelkasten"
  },
  {
    "objectID": "posts/2019-04-12-aws-solutions-architect-professional-study-guide/2019-04-12-aws-solutions-architect-professional-study-guide.html",
    "href": "posts/2019-04-12-aws-solutions-architect-professional-study-guide/2019-04-12-aws-solutions-architect-professional-study-guide.html",
    "title": "AWS Solutions Architect Professional Study Guide",
    "section": "",
    "text": "Abstract\nThese are my raw format notes that I took while studying for the AWS Solutions Architect - Professional certifcation. I passed the test and am officially certified!\nDomain 1: Design for Organizational Complexity 12.5%\n1.1. Determine cross-account authentication and access strategy for complex organizations (for example, an organization with varying compliance requirements, multiple business units, and varying scalability requirements).\n1.2. Determine how to design networks for complex organizations (for example, an organization with varying compliance requirements, multiple business units, and varying scalability requirements).\n1.3. Determine how to design a multi-account AWS environment for complex organizations (for example, an organization with varying compliance requirements, multiple business units, and varying scalability requirements).\n\nProhibit the use of unapproved services in production AWS accounts and minimize additional management overhead as the number of accounts increases\n\nAWS Organizations\n\nconsolidated billing or “all features”?\n\nProvides single payer and centralized cost tracking\n\nboth\n\nLets you create and invite accounts\n\nboth\n\nAllows you to apply policy-based controls\n\nall features only\nservice control policies\n\nplaces onto Organizational Units (OUs) within AWS Organizations\n\n\nHelps you simplify organization-wide management of AWS services\n\nall features only\n\n\n\n\nDifferent teams all have their own non-production accounts with AWS Organizations. Constraints need to be put in place to control costs without affecting IAM permissions.\n\nWhere should the AWS Budget be created?\n\nmaster account or development account?\n\nmaster account can create budgets and alerting for linked accounts\n\n\nforecasted budget reaches 100% triggers a Lambda. Lambda applies a SCP to deny new infrastructure\n\nVPC A is peered with VPC B and VPC C. VPC B and VPC C have matching CIDR blocks, and their subnets have matching CIDR blocks. The route table for subnet B in VPC B points to the VPC peering connection pcx-aaaabbbb to access the VPC A subnet. The VPC A route table is configured to send 10.0.0.0/16 traffic to peering connection pcx-aaaaccccc.\n\nRoute Table | Destination | Target | | |\n|-||–||| | Subnet B in VPC B | 10.0.0.0/16 | Local | | | | | 172.16.0.0/24 | pcx-aaaabbbb | | | | VPC A | 172.16.0.0/24 | Local | | | | | 10.0.0.0/16 | pcx-aaaacccc | | |\n\nAWS currently does not support unicast reverse path forwarding in VPC peering connections that checks the source IP of packets and routes reply packets back to the source.\nIf subnet B in VPC C has an instance with an IP address of 10.0.1.66/32, it receives the response traffic from VPC A. The instance in subnet B in VPC B does not receive a response to its request to VPC A because VPC A will route the response to VPC C.\nTo prevent this, you can add a specific route to VPC A’s route table with a destination of 10.0.1.0/24 and a target of pcx-aaaabbbb. The route for 10.0.1.0/24 traffic is more specific, therefore traffic destined for the 10.0.1.0/24 IP address range goes via VPC peering connection pcx-aaaabbbb\nThe key take away is that more specific routes (meaning they map to smaller CIDR ranges) will always take precedence.\n\nDomain 2: Design for New Solutions 31%\n2.1. Determine security requirements and controls when designing and implementing a solution.\n2.2. Determine a solution design and implementation strategy to meet reliability requirements.\n2.3. Determine a solution design to ensure business continuity.\n2.4. Determine a solution design to meet performance objectives.\n2.5. Determine a deployment strategy to meet business requirements when designing and implementing a solution.\n\nyou are designing an application that will perform several calculations each night. the calculations are independent of each other and may take several hours to complete. what design will minimize costs, minimize interdependencies, and execute the calculations in parallel?\n\nAWS Batch\n\nintegrates with EC2, Spot Fleet, and ECS\n\n\nyou are designing an automated DR plan for a multi-tier web application. it must failover to a second region with an RTO of 30 minutes and an RPO of 15 minutes. there is a health check alarm that publishes an SNS notification if the application fails.\n\nAWS Lambda subscriptions to SNS\n\nchange the desired capacity of your pilot light auto scaling group in the 2nd region.\npromote the RDS read replica in the second region to a standalone DB instance.\n\n\nyou want to monitor and analyze network traffic for possible threats. what solution requires minimal development and administration, scales to accommodate large amounts of network traffic, and allows queries and visualizations of the data?\n\nVPC flow logs\n\nall traffic will be published to CloudWatch log stream. but it needs to be archived in S3 in for better flexibility and control over log retention and the analysis.\n\nKinesis Data Firehose can push the logs to S3. Kinesis is great because it will happen in near real-time versus a scheduled migration.\nAthena can create an external table and query the logs using SQL syntax.\nQuickSight can connect to Athena to visualize the data.\n\nyour application uses a log stream. each record in the stream may contain up to 400 KB od data. design a solution to caputure a subset of metrics from the stream to be analyzed for trends over time using complex SQL queries.\n\nKinesis Data Streams versus Kinesis Firehose\n\nThere are two major differences:\n\nStreams requires capacity provisioning. You must select a number of shards (i.e., ingestion throughput), whereas Firehose will scale automatically.\nFirehose can only stream into S3, Elasticsearch, and Redshift. Kinesis Data Streams can be leveraged to stream into any custom application.\n\nKinesis Analytics\n\nWith Amazon Kinesis Data Analytics for SQL Applications, you can process and analyze streaming data using standard SQL. The service enables you to quickly author and run powerful SQL code against streaming sources to perform time series analytics, feed real-time dashboards, and create real-time metrics.\n\n\n\nyou’re deploying a database server cluster that requires minimum network latency between nodes and maximum network throughput. What features in EC2 will be useful?\n\nEnhanced Networking\n\nElastic Network Adapter (ENA)\n\nup to 100 GB for supported EC2 instance types\n\nIntel 82599 Virtual Function (VF) interface\n\nup to 10 GB for supported EC2 instance types\n\n\nJumbo Frames\n\nThe maximum transmission unit (MTU) of a network connection is the size, in bytes, of the largest permissible packet that can be passed over the connection. The larger the MTU of a connection, the more data that can be passed in a single packet.\nJumbo frames allow more than 1500 bytes of data by increasing the payload size per packet, and thus increasing the percentage of the packet that is not packet overhead. Fewer packets are needed to send the same amount of usable data.\nFor instances that are collocated inside a cluster placement group, jumbo frames help to achieve the maximum network throughput possible, and they are recommended in this case.\n\nPlacement Groups\n\ncluster\n\nall instances are within the same AZ and can span peered VPCs\nThe chief benefit of a cluster placement group, in addition to a 10 Gbps flow limit, is the non-blocking, non-oversubscribed, fully bi-sectional nature of the connectivity. In other words, all nodes within the placement group can talk to all other nodes within the placement group at the full line rate of 10 Gbps flows and 25 aggregate without any slowing due to over-subscription.\n\nspread\n\nA spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source.\nmulti-az\nmax of 7 instances\n\npartition\n\nsimilar to spread, but instances are grouped into partitions rather than each individual instance running on its own rack\nmax of 7 partitions\n\n\n\nyou are migrating an application that uses an API key which is stored in a local file. after the migration, the application will be run on EC2 instances and the API key must be more secure. The API key should be unique for each environment, access requests should be auditable, it should be encrypted at rest, and access permissions should be granular.\n\nAWS Systems Manager Parameter Store + KMS\n\nSecureString\n\nuses default aws/ssm KMS key or a specified key\n\nSSM permissions are distinct from key policy permissions\nAll API calls will be recorded in CloudTrail\n\n\n\nDomain 3: Migration Planning 15%\n3.1. Select existing workloads and processes for potential migration to the cloud.\n3.2. Select migration tools and/or services for new and migrated solutions based on detailed AWS knowledge.\n3.3. Determine a new cloud architecture for an existing solution.\n3.4. Determine a strategy for migrating existing on-premises workloads to the cloud.\n\nYou are migrating a MySQL database. The database is forecasted to grow and the company wants to reduce the administrative/operational burden of maintaining it. What is a cost-effective solution that can be implemented quickly, requires minimal administration, and offers high performance now and in the future? Must be HA and remain operational during the migration.\n\nAurora vs RDS\n\nAurora is a fully managed relational database engine that’s compatible with MySQL and PostgreSQL.\nRDS is a web service that makes it easier to set up, operate, and scale a relational database in the cloud.\n\nSupports MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL\n\nAurora will automatically scale storage - you do not need to plan for capacity like you do with RDS.\nAurora can provision up to 15 replicas. RDS MySQL can have 5.\nFailover on Aurora is automatic. Failover is a manual process on RDS.\n\nAWS Database Migration Service (AWS DMS)\n\nAt its most basic level, AWS DMS is a server in the AWS Cloud that runs replication software. You create a source and target connection to tell AWS DMS where to extract from and load to. Then you schedule a task that runs on this server to move your data.\nWith AWS DMS, you can perform one-time migrations, and you can replicate ongoing changes to keep sources and targets in sync.\nIf you want to change database engines, you can use the AWS Schema Conversion Tool (AWS SCT) to translate your database schema to the new platform.\n\n\nYou must regularly transfer data from on-prem to S3. Data must be encrypted in-transit and at-rest. Data cannot traverse the public internet. The bucket can only be accessed from on-prem network and VPC.\n\nDirect connect with private VIF\nCreate an S3 VPC endpoint. This prevents S3 traffic from going to the usual, public endpoint over the internet.\nConfigure bucket policy to deny non-VPC traffic and to require TLS/SSL.\nuse load balanced ec2 to proxy traffic from on-prem to the s3 vpc endpoint\n\na company wants to migrate a multi-tier web app with minimal changes to the code, cost, and platform management. the stack consists of a hardware load balancer, 2 apache hosts, 4 Java/Tomcat application servers, and a MySQL server.\n\nDomain 4: Cost Control 12.5%\n4.1. Select a cost-effective pricing model for a solution.\n4.2. Determine which controls to design and implement that will ensure cost optimization.\n4.3. Identify opportunities to reduce cost in an existing solution.\n\nan application receives data every hour, on the hour. the data is processed for 50 minutes and produces a 10GB output. This output is heavily accessed during the first hour it is available with useage dropping as new outputs become available. what’s the MOST cost-effective architecture?\n\nAbsolute cheapest way for compute would be EC2 1-hour Spot blocks in multiple regions\nCheapest storage class would be S3 One Zone-Infrequent Access with a Lifecycle policy to transition to Glacier or S3 Deep Glacier Archive\n\nA company is migrating all of its data to S3 within the next 4 weeks. There is 900 TB and a 100 Mbps internet link. Up to 20% of the throughput is regularly used by existing systems. Whats the MOST cost-effective way to migrate the data in time?\n\nOrder multiple Snowballs\n\n\nDomain 5: Continuous Improvement for Existing Solutions 29%\n5.1. Troubleshoot solution architectures.\n5.2. Determine a strategy to improve an existing solution for operational excellence.\n5.3. Determine a strategy to improve the reliability of an existing solution.\n5.4. Determine a strategy to improve the performance of an existing solution.\n5.5. Determine a strategy to improve the security of an existing solution.\n5.6. Determine how to improve the deployment of an existing solution.\n\nYour application accesses a on-premises database over a 1 Gbps AWS Direct Connect link. The database is also used by applications in the data center. What can you do to make your architecture better?\n\nDirect Connect is a single point of failure. If you do not have multiple Direct Connect connections, you should also establish a VPN connection between the data center and the virtual private gateway in the VPC.\n\nYour application sends logs to CloudWatch and also has a DynamoDB table to record the number of times different operations are invoked. The number of GET invocations are much higher in the logs than what is recorded in the DynamoDB table.\n\nUse conditional writes when the UpdateItem call saves the incremented amount.\n\nan application uploads files greater than 10 GB to S3, but far away locations have performance issues.\n\nEnable S3 transfer acceleration on the bucket\nUse multi-part upload\ndo not worry about random prefixes. This is no longer a performance bottleneck in S3 for reads or writes.\n\nAn application runs its database on an RDS instance. During peak usage periods, some user requests time out. The CloudWatch DiskQueueDepth metric spikes during these periods. How can you improve the application?\n\nModify RDS instance storage type to Provisioned IOPS SSD (io1) and provision maximum IOPS.\n\nYour web app sits behind an ELB ALB. There have been spikes in traffic that cause the app to slow down and fail. Logs reveal the additional traffic contained malformed request from multiple sources. What solution will MOST quickly block these types of attacks? AKA “how to minimize the impact of a DDoS attack?”\n\nAWS Shield\n\nAWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. AWS Shield Standard is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost.\n\nAWS WAF\n\nAWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. You can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications. If you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules.\n\nRoute 53\n\nOne of the most common targets of DDoS attacks is the Domain Name System (DNS). Amazon Route 53 is a highly available and scalable DNS service designed to route end users to infrastructure running inside or outside of AWS. Route 53 makes it possible to manage traffic globally through a variety of routing types, and provides out-of-the-box shuffle sharding and Anycast routing capabilities to protect domain names from DNS-based DDoS attacks.\n\nCloudFront\n\nAmazon CloudFront distributes traffic across multiple edge locations and filters requests to ensure that only valid HTTP(S) requests will be forwarded to backend hosts. CloudFront also supports geoblocking, which you can use to prevent requests from particular geographic locations from being served.\n\nELB\n\nElastic Load Balancing, like CloudFront, only supports valid TCP requests, so DDoS attacks such as UDP and SYN floods are not able to reach EC2 instances."
  },
  {
    "objectID": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html",
    "href": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html",
    "title": "TL;DR - Kubernetes Up & Running, Part 4",
    "section": "",
    "text": "These are my notes from reading Kubernetes Up & Running by Kelsey Hightower, Brendan Burns, and Joe Beda. Kelsey Hightower is a Staff Developer Advocate for the Google Cloud Platform. Brendan Burns is a Distinguished Engineer in Microsoft Azure and cofounded the Kubernetes project at Google. Joe Beda is the CTO of Heptio and cofounded the Kubernetes project, as well as Google Compute Engine.\nThis is a phenomenal book that covers both the whys and hows of Kubernetes. I read the 1st edition, but a 2nd edition is coming out soon. I’m using this as study material for my CKAD and CKA certifications.\nThis article is part of a series. You can read Part 1, Part 2, and Part 3.\n\n\nMuch like how ReplicaSets manage the Pods beneath them, the Deployment object manages ReplicaSets beneath it. Deployments are used to manage the release of new versions and roll those changes out in a simple, reliable fashion. Deployments are a top-level object when compared to ReplicaSets. This means that if you scale a ReplicaSet, the Deployment controller will scale back to the desired state defined in the Deployment, not in the ReplicaSet.\nDeployments revolve around their ability to perform a rollout. Rollouts are able to be paused, resumed, and undone. You can undo both partial and completed rollouts. Additionally, the rollout history of a Deployment is retained within the object and you can rollback to a specific version. For Deployments that are long-running, it’s a best practice to limit the size of the revision history so that the Deployment object does not become bloated. For example, if you rollout changes every day and you need 2 weeks of revision history, you would set spec.revisionHistoryLimit to 14. Undoing a rollout (i.e., rolling back) follows all the same policies as the rollout strategy.\nBecause Deployments make it easy to roll back and forth between versions, it is absolutely paramount that each version of your application is capable of working interchangeably with both slightly older and slightly newer versions. This backwards and forwards compatibility is critical for decoupled, distributed systems and frequent deployments.\nDeployments can have two different values for .spec.strategy.type: Recreate or RollingUpdate. If .spec.strategy.type==Recreate, the Deployment will terminate all Pods associated with it and the associated ReplicaSet will re-create them. This is a fast and simple approach, but results in downtime. It should only be used in testing. RollingUpdate is much more sophisticated and is the default configuration. RollingUpdate can be configured using 2 different parameters/approaches: 1. maxUnavailable: this parameter can be set as an absolute number or a percentage. If it is set to a value of 1, a single Pod will be terminated and re-created using the new version. After establishing that the Pod is ready, the rollout will proceed to the next Pod. This decreases capacity by the parameter value at any given time. 2. maxSurge: this parameter can be set as an absolute number or a percentage. If it is set to a value of 1, a single Pod will be created using the new version. After establishing that the Pod is ready, Pod from the previous version will be deleted. This increases capacity by the parameter value at any given time.\nBonus material: It is not explicitly mentioned in the book, but you can combine these two parameters. In fact, the default setting is 25% for both.\nWhen performing a rollout, the Deployment controller needs to determine if a Pod is ready before moving on to the next Pod. This means that you have to specify readiness checks in your Pod templates. Beyond this, Deployments also support the minReadSeconds parameter. This is a waiting period that begins after the Pod is marked as ready. minReadySeconds can help catch bugs that take a few minutes to show up (e.g., memory leaks). Similar to minReadSeconds, the parameter progressDeadlineSeconds is used to define a timeout limit for the deployment. It’s important to note that this timer is measured by progress, not overall length of the rollout. In this context, progress is defined as any time the deployment creates or deletes a Pod. When that happen, the progressDeadlineSeconds timer resets. If the deployment does timeout, it is marked as a failure.\nBonus material: The following is not explained explicitly in the book, but is available in the documentation.\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the Deployment’s .status.conditions:\nType=Progressing\nStatus=False\nReason=ProgressDeadlineExceeded\nNote: Kubernetes takes no action on a stalled Deployment other than to report a status condition with Reason=ProgressDeadlineExceeded. Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the Deployment to its previous version.\n\n\n\nDecoupling state from your applications and applying a microservice architecture allows you to achieve incredible scale and reliability, but it does not remove the need for state. Kubernetes has several ways to store or access state depending on the needs of the application.\nImporting External Services\nIf your database, or any other service, is running outside of the Kubernetes cluster, it’s worthwhile to be able to represent this service using native k8s API definitions. By representing the service as an object within Kubernetes, you can maintain identical configurations between environments by using namespaces. A simple example is using namespace: test for your k8s-native proxy/testing services, but using namespace: prod to point to the production database that is running outside of the cluster on-premise or somewhere else. For a typical Service, a ClusterIP is provisioned and kube-dns creates an A record to route to the Service. If we need to route to the DNS name of an external service, we can use the ExternalName type to have kube-dns create a CNAME record instead.\nkind: Service\napiVersion: v1\nmetadata:  \n  name: external-database\nspec:\n  type: ExternalName\n  externalName: \"external.database.galenballew.fyi\"\nIf the external database is only accessible via an IP address (or multiple IP addresses) you can create a Service without a spec (i.e., without a label selector and without ExternalName type). This will create a ClusterIP for the service and an A record, but there will be no IP addresses to load balance to. You will need to manually create an Endpoints object and associate it with the Service. If the IP address or addresses change, you are also responsible for updating the Endpoints object. You are also responsible for all health checks for external services and how your application will handle unavailability.\nRunning Reliable Singletons\nRunning a storage solution on a single Pod, VM, or server trades the complexity of distributing the data for the risk of downtime. Within Kubernetes, we can use k8s primitives to run singletons with some measure of reliability by combining ReplicaSet, PersistentVolume, and PersistentVolumeClaim objects. The actual disk is represented using a PersistentVolume. Kubernetes provides drivers for all the major public cloud providers - you just provide the type in the spec and k8s handles the rest. A PersistentVolumeClaim is used decouple our Pod definition from the storage definition. In this way, a Pod manifest can be cloud agnostic by referencing a PersistentVolumeClaim that is composed of PersistentVolumes of various types/providers. Similarly, if we want to decouple our PersistentVolumeClaim from specific, pre-existing PersistentVolumes, we can define a StorageClass object that can be referenced by a PersistentVolumeClaim. This object allows k8s operators to create disk on-demand and enables dynamic volume provisioning.\nStatefulSets\nStatefulSets are very similar to ReplicaSets, except for 3 differences:\n1. Each replica gets a persistent hostname with a unique index instead of the random suffix usually attached by the ReplicaSet controller (e.g., database-0, database-1, …, database-n)\n2. Each replica is created in order from lowest to highest index. Creation is blocked until the preceding replica is healthy and available instead of creating all the replicas in parallel. This also applies to scaling up.\n3. When deleted, each replica is deleted in order from highest to lowest index. This also applies to scaling down.\nWhen you create a StatefulSet, you will need to create a “headless” Service to manage it: a Service that does not provision a cluster virtual IP address. Since each replica in the StatefulSet is unique, it doesn’t make sense to have a load-balancing IP for them. To create a headless Service, simply use clusterIP: None in the specification. After the service is created, a DNS entry will be created for each unique replica, as well as a DNS entry for the StatefulSet itself that contains the addresses of all the replicas. These well-defined, persistent names for each replica and the ability to route to them is critical when configuring a replicated storage solution. For actual disk, StatefulSets will need to use volumeClaimTemplates since there will be multiple replicas (they can’t all use the same unique volume claim). The volume claim template can be configured to reference a StorageClass to enable dynamic provisioning.\nBonus material: Operators are incredibly useful in Kubernetes. They include the logic needed to have applications behave as desired within Kubernetes (e.g., scaling, sharding, and promotion for a distributed database). Check out Awesome Operators to see examples.\n\n\n\nThis chapter presents 3 different walk-throughs of deploying real-world applications. The applications are Parse, Ghost, and Redis.\n\n\n\nThe appendix includes instructons on how to set up a cluster of Raspberry Pi devices and install Kubernetes on them."
  },
  {
    "objectID": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#chapter-12-deployments",
    "href": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#chapter-12-deployments",
    "title": "TL;DR - Kubernetes Up & Running, Part 4",
    "section": "",
    "text": "Much like how ReplicaSets manage the Pods beneath them, the Deployment object manages ReplicaSets beneath it. Deployments are used to manage the release of new versions and roll those changes out in a simple, reliable fashion. Deployments are a top-level object when compared to ReplicaSets. This means that if you scale a ReplicaSet, the Deployment controller will scale back to the desired state defined in the Deployment, not in the ReplicaSet.\nDeployments revolve around their ability to perform a rollout. Rollouts are able to be paused, resumed, and undone. You can undo both partial and completed rollouts. Additionally, the rollout history of a Deployment is retained within the object and you can rollback to a specific version. For Deployments that are long-running, it’s a best practice to limit the size of the revision history so that the Deployment object does not become bloated. For example, if you rollout changes every day and you need 2 weeks of revision history, you would set spec.revisionHistoryLimit to 14. Undoing a rollout (i.e., rolling back) follows all the same policies as the rollout strategy.\nBecause Deployments make it easy to roll back and forth between versions, it is absolutely paramount that each version of your application is capable of working interchangeably with both slightly older and slightly newer versions. This backwards and forwards compatibility is critical for decoupled, distributed systems and frequent deployments.\nDeployments can have two different values for .spec.strategy.type: Recreate or RollingUpdate. If .spec.strategy.type==Recreate, the Deployment will terminate all Pods associated with it and the associated ReplicaSet will re-create them. This is a fast and simple approach, but results in downtime. It should only be used in testing. RollingUpdate is much more sophisticated and is the default configuration. RollingUpdate can be configured using 2 different parameters/approaches: 1. maxUnavailable: this parameter can be set as an absolute number or a percentage. If it is set to a value of 1, a single Pod will be terminated and re-created using the new version. After establishing that the Pod is ready, the rollout will proceed to the next Pod. This decreases capacity by the parameter value at any given time. 2. maxSurge: this parameter can be set as an absolute number or a percentage. If it is set to a value of 1, a single Pod will be created using the new version. After establishing that the Pod is ready, Pod from the previous version will be deleted. This increases capacity by the parameter value at any given time.\nBonus material: It is not explicitly mentioned in the book, but you can combine these two parameters. In fact, the default setting is 25% for both.\nWhen performing a rollout, the Deployment controller needs to determine if a Pod is ready before moving on to the next Pod. This means that you have to specify readiness checks in your Pod templates. Beyond this, Deployments also support the minReadSeconds parameter. This is a waiting period that begins after the Pod is marked as ready. minReadySeconds can help catch bugs that take a few minutes to show up (e.g., memory leaks). Similar to minReadSeconds, the parameter progressDeadlineSeconds is used to define a timeout limit for the deployment. It’s important to note that this timer is measured by progress, not overall length of the rollout. In this context, progress is defined as any time the deployment creates or deletes a Pod. When that happen, the progressDeadlineSeconds timer resets. If the deployment does timeout, it is marked as a failure.\nBonus material: The following is not explained explicitly in the book, but is available in the documentation.\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the Deployment’s .status.conditions:\nType=Progressing\nStatus=False\nReason=ProgressDeadlineExceeded\nNote: Kubernetes takes no action on a stalled Deployment other than to report a status condition with Reason=ProgressDeadlineExceeded. Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the Deployment to its previous version."
  },
  {
    "objectID": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#chapter-13-integrating-storage-solutions-and-kubernetes",
    "href": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#chapter-13-integrating-storage-solutions-and-kubernetes",
    "title": "TL;DR - Kubernetes Up & Running, Part 4",
    "section": "",
    "text": "Decoupling state from your applications and applying a microservice architecture allows you to achieve incredible scale and reliability, but it does not remove the need for state. Kubernetes has several ways to store or access state depending on the needs of the application.\nImporting External Services\nIf your database, or any other service, is running outside of the Kubernetes cluster, it’s worthwhile to be able to represent this service using native k8s API definitions. By representing the service as an object within Kubernetes, you can maintain identical configurations between environments by using namespaces. A simple example is using namespace: test for your k8s-native proxy/testing services, but using namespace: prod to point to the production database that is running outside of the cluster on-premise or somewhere else. For a typical Service, a ClusterIP is provisioned and kube-dns creates an A record to route to the Service. If we need to route to the DNS name of an external service, we can use the ExternalName type to have kube-dns create a CNAME record instead.\nkind: Service\napiVersion: v1\nmetadata:  \n  name: external-database\nspec:\n  type: ExternalName\n  externalName: \"external.database.galenballew.fyi\"\nIf the external database is only accessible via an IP address (or multiple IP addresses) you can create a Service without a spec (i.e., without a label selector and without ExternalName type). This will create a ClusterIP for the service and an A record, but there will be no IP addresses to load balance to. You will need to manually create an Endpoints object and associate it with the Service. If the IP address or addresses change, you are also responsible for updating the Endpoints object. You are also responsible for all health checks for external services and how your application will handle unavailability.\nRunning Reliable Singletons\nRunning a storage solution on a single Pod, VM, or server trades the complexity of distributing the data for the risk of downtime. Within Kubernetes, we can use k8s primitives to run singletons with some measure of reliability by combining ReplicaSet, PersistentVolume, and PersistentVolumeClaim objects. The actual disk is represented using a PersistentVolume. Kubernetes provides drivers for all the major public cloud providers - you just provide the type in the spec and k8s handles the rest. A PersistentVolumeClaim is used decouple our Pod definition from the storage definition. In this way, a Pod manifest can be cloud agnostic by referencing a PersistentVolumeClaim that is composed of PersistentVolumes of various types/providers. Similarly, if we want to decouple our PersistentVolumeClaim from specific, pre-existing PersistentVolumes, we can define a StorageClass object that can be referenced by a PersistentVolumeClaim. This object allows k8s operators to create disk on-demand and enables dynamic volume provisioning.\nStatefulSets\nStatefulSets are very similar to ReplicaSets, except for 3 differences:\n1. Each replica gets a persistent hostname with a unique index instead of the random suffix usually attached by the ReplicaSet controller (e.g., database-0, database-1, …, database-n)\n2. Each replica is created in order from lowest to highest index. Creation is blocked until the preceding replica is healthy and available instead of creating all the replicas in parallel. This also applies to scaling up.\n3. When deleted, each replica is deleted in order from highest to lowest index. This also applies to scaling down.\nWhen you create a StatefulSet, you will need to create a “headless” Service to manage it: a Service that does not provision a cluster virtual IP address. Since each replica in the StatefulSet is unique, it doesn’t make sense to have a load-balancing IP for them. To create a headless Service, simply use clusterIP: None in the specification. After the service is created, a DNS entry will be created for each unique replica, as well as a DNS entry for the StatefulSet itself that contains the addresses of all the replicas. These well-defined, persistent names for each replica and the ability to route to them is critical when configuring a replicated storage solution. For actual disk, StatefulSets will need to use volumeClaimTemplates since there will be multiple replicas (they can’t all use the same unique volume claim). The volume claim template can be configured to reference a StorageClass to enable dynamic provisioning.\nBonus material: Operators are incredibly useful in Kubernetes. They include the logic needed to have applications behave as desired within Kubernetes (e.g., scaling, sharding, and promotion for a distributed database). Check out Awesome Operators to see examples."
  },
  {
    "objectID": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#chapter-14-deploying-real-world-applications",
    "href": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#chapter-14-deploying-real-world-applications",
    "title": "TL;DR - Kubernetes Up & Running, Part 4",
    "section": "",
    "text": "This chapter presents 3 different walk-throughs of deploying real-world applications. The applications are Parse, Ghost, and Redis."
  },
  {
    "objectID": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#appendix-a-building-a-raspberry-pi-kubernetes-cluster",
    "href": "posts/2019-09-15-kuar-part4/2019-09-15-kuar-part4.html#appendix-a-building-a-raspberry-pi-kubernetes-cluster",
    "title": "TL;DR - Kubernetes Up & Running, Part 4",
    "section": "",
    "text": "The appendix includes instructons on how to set up a cluster of Raspberry Pi devices and install Kubernetes on them."
  },
  {
    "objectID": "posts/2015-12-05-metaselfie/2015-12-05-metaselfie.html",
    "href": "posts/2015-12-05-metaselfie/2015-12-05-metaselfie.html",
    "title": "Metaselfie",
    "section": "",
    "text": "This repository contains the build files for Metaselfie.\n3d modeling, animation, and compositing by Galen Ballew\nSound production by Fractalstein"
  },
  {
    "objectID": "posts/2016-05-25-elliptic-curves/2016-05-25-elliptic-curves.html",
    "href": "posts/2016-05-25-elliptic-curves/2016-05-25-elliptic-curves.html",
    "title": "Elliptic Curves",
    "section": "",
    "text": "Elliptic curves with complex multiplication give rise to tori over the complex numbers\nCompleted in spring 2016 at the Mathematical Computing Laboratory at UIC.\nThis repository contains the paper, poster, images, and 3d modeling files from my research and learning project on elliptic curves. I completed this project with James Duncan under the guidance of Cara Mullen (PhD) and Professor Alina Cojocaru (PhD).\nDownload Paper Download Poster\n\n\nSummary\nOur research focuses on elliptic curves $E over $Q with complex multiplication (by the maximal order of an imaginary quadratic field). Viewed over $C, each $E gives rise to two tori, defined by the generators $_1 and $_2 of the period lattice. These tori can be constructed virtually into a 3D mesh. Further, this mesh can be translated into gcode and printed using a 3D printer."
  },
  {
    "objectID": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html",
    "href": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html",
    "title": "TL;DR - Kubernetes Up & Running, Part 2",
    "section": "",
    "text": "These are my notes from reading Kubernetes Up & Running by Kelsey Hightower, Brendan Burns, and Joe Beda. Kelsey Hightower is a Staff Developer Advocate for the Google Cloud Platform. Brendan Burns is a Distinguished Engineer in Microsoft Azure and cofounded the Kubernetes project at Google. Joe Beda is the CTO of Heptio and cofounded the Kubernetes project, as well as Google Compute Engine.\nThis is a phenomenal book that covers both the whys and hows of Kubernetes. I read the 1st edition, but a 2nd edition is coming out soon. I’m using this as study material for my CKAD and CKA certifications.\nThis article is part of a series. You can read Part 1 here.\nNote to the reader: Throughout these articles, I am loose with my formatting of Kubernetes objects, especially in the case of Service versus Service. This is to avoid confusion with the common term of service. Constant width formatting is typically reserved for program elements, such as k8s API objects, parameters, or processes.\n\n\nPods represent a collection of application containers and volumes running in the same execution environment. Pods are the most atomic object within Kubernetes, therefore all containers within a Pod will always land on the same node within a cluster. Each container runs in its own cgroup and we will leverage that using requests and limits later on in this chapter. All applications running in a Pod share the same hostname, IP address and port space. Containers within a Pod can communicate with one another, but are isolated from containers in other Pods. Since a Pod can be composed of multiple containers, it’s natural to ask which containers should be included in any given Pod. The question to ask yourself is “Will these containers work correctly if they land on separate machines?” If the answer is “no”, then the containers should be placed in the same Pod. As we will see, it’s often the case that containers within the same Pod interact via a shared volume that is also within the Pod.\nPods are managed via YAML or JSON templates known as a Pod manifest. These manifests are text-file representations of the k8s API object. They are sent to kube-apiserver, which in turn passes them to the kube-scheduler. The scheduler then places the Pod onto a node that has sufficient resources. A daemon on the node, named kubelet, creates the containers declared in the Pod manifest and performs any health checks. Once a Pod as been scheduled to a node, there is no rescheduling if the node fails. The Kubernetes scheduler takes this into account and tries to ensure that Pods from the same application are distributed onto different nodes, in order to avoid a single failure domain. Once scheduled, will only change nodes if they are explicitly destroyed and rescheduled. When a Pod is deleted, it has a termination grace period with a default value of 30 seconds. This grace period allows the Pod to finish serving any active requests that it is processing before terminating. Any data stored within the containers on the Pod will be lost when the Pod terminates. The PersistentVolume object can be used to store data across multiple instances of Pod and their subsequent lifespans.\nOnce a Pod is running, you may want to access it even if it is not serving traffic on the Internet. You can use port forwarding to create a secure tunnel from your local machine, through the Kubernetes master, to the instance of the Pod running on a worker node.\nThere are several types of health checks for containers within Kubernetes. The first is a process health check. This health check simply checks if the main process of your application is running. If it isn’t, Kubernetes restarts it. This health check is automatic and does not need to be defined. A simple process check is often insufficient however. Imagine that your process is deadlocked - the health check will be green, but your application will be red. To address this, Kubernetes supports liveness health checks. Liveness health checks are defined within the Pod manifest per container and each container is health checked separately. They run application-specific logic to ensure that the application is not only running, but functioning properly. Lastly, there are readiness health checks. These checks are configured similarly to liveness probes. The difference is that containers which fail a liveness health check are restarted while containers that file a readiness probe are removed from Service load balancers. Combining liveness and readiness probes ensures that all traffic is routed to healthy containers that have capacity to fulfill the request. Kubernetes also supports tcpSocket health checks, which are useful for databases r other non-HTTP-based APIs. Finally, k8s supports exec probes. These execute a script in the context of a container - if the script returns zero, the probe succeeds. These are useful for applications where validation logic doesn’t fit neatly into an HTTP request.\nOne of the most powerful features of Pods is the ability to define the minimum and maximum compute resources available for the containers within the Pod. By rightsizing container requirements, kube-scheduler is able to efficiently pack Pods onto nodes, thereby driving up utilization. Requests specify the minimum resources needed by a Pod in order to be scheduled - limits specify the maximum resources a container may use. Both are declared at the container level within the Pod manifest. It’s worth mentioning that memory (unlike CPU) cannot be redistributed if it has already been allocated to a container process. Therefore, when the system runs out of memory, kubelet terminates containers whose memory usage is greater than their requested amount.\nVolumes are used for containers within a Pod to share some kind of state information. They can be used to communicate, synchronize, cache, or even persist state data beyond the lifespan of the Pod itself. Volumes are declared within the Pod manifest and container declarations may include volumeMounts. It’s important to note that containers can mount the same volume to different paths. emptyDir volumes can be used to create a shared filesystem between containers. hostDir volumes can grant access to the underlying host filesystem. There are multiple supported protocols for remote network storage volumes that can be used for truly persistent data, including NFS, iSCSI, and cloud network storage like AWS EBS, Azure FDS, and GCP Persistent Disk.\n\n\n\nLabels and annotations are types of metadata about Kubernetes API objects. They each serve a different purpose. Labels are used to identify and select sets of objects, while annotations are designed to hold nonidentifying information that can be leveraged by tools and libraries.\nKubernetes uses labels to group and select objects. There are a variety of ways to express the selection logic:\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\nkey=value\nkey is set to value\n\n\nkey!=value\nkey is not set to value\n\n\nkey in (value1, value2)\nkey is one of value1 or value2\n\n\nkey notin (value1, value2)\nkey is not one of value1 or value2\n\n\nkey\nkey is set\n\n\n!key\nkey is not set\n\n\n\nYou can use kubectl and labels to find and manage objects. Kubernetes itself also uses labels to accomplish the same thing.\nAnnotations are key-value pairs, much like labels, but they have less restrictions around string formatting. While this makes them exceedingly useful, it also means that there is no guarantee that the data contained in the annotation is valid. The nature/formatting of the data contained in the key-value pair can dictate whether it has to be an annotation or not. Outside of that, it’s a good practice to add information about an object as an annotation and promote it to a label if you find yourself wanting to use it in a selector.\nAnnotations can be used to:\n- Store comments about the latest change to an object.\n- Communicate a special scheduling policy to a specialized scheduler. - Build, release, and image information from source version control. - Prototype alpha functionality of the Kubernetes API. - Track status during rolling deployments and provide the necessary information needed to roll back to a previous version if required. - This is the primary use case for annotations.\n\n\n\nService objects are used to expose Pods within a cluster and load-balance across them. Usually, a Service uses a named label selector to identify the Pods, but it is also possible to define a Service without selectors. By default, when a Service is created, k8s assigns it a virtual IP called a cluster IP. The cluster IP load-balances across the selected Pods. It also has a DNS name within kube-dns. In true k8s-building-on-k8s fashion, kube-dns is a Service with a ClusterIP. Because Service objects are responsible for load-balancing, they also handle the management of readiness checks. Besides ClusterIP, there are a few other ServiceTypes.\nNodePort builds upon ClusterIP and is used to serve traffic from outside of the cluster. In addition to a ClusterIP, kube-apiserver assigns a port to the Service (or you can specify the port yourself) and all nodes will forward traffic to that port to the Service. This means that if you can reach any node in the cluster, you can contact a service without knowing where that service is running. LoadBalancer builds upon NodePort. It integrates with cloud providers to provision a new load balancer and direct it at the nodes in your cluster, resulting in an IP or hostname depending on the cloud provider. Bonus material: there is also ExternalName if your cluster is running CoreDNS v1.7 or higher.\nFor every Service object, Kubernetes creates an Endpoints object which contains the IP addresses for that service. Kubernetes uses Endpoints to consume services rather than a cluster IP - which means your applications can too! By talking directly the k8s API, you can retrieve and call service endpoints. Kubernetes can also “watch” objects and be notified when they change, including the IPs associated with a service. Most applications are not written to be Kubernetes-native and therefore don’t leverage this, but it’s important to know that it’s possible.\nRegardless of the type, all Service objects will load-balance. This is made possible by a component named kube-proxy that runs on every node in the cluster. kube-proxy watches for new services via kube-apiserver. It programs a set of iptables rules (abstracted as ClusterIP) on in the kernel of the host, which change the destination of packets to one of the endpoints for that service. Whenever the set of endpoints changes, the set of iptables rules are rewritten. It is possible for a user to specify a specific cluster IP when creating a service. Once created, the cluster IP cannot be changed without deleting and rebuilding the Service object. In order to be assigned, a cluster IP must come from the cluster IP range defined by kube-apiserver and not already be in use. It’s possible to configure the service address range using the --service-cluster-ip-range flag on the kube-apiserver binary. The range should not overlap with the IP subnets and ranges assigned to each Docker bridge or Kubernetes node.\n\n\n\nReplicaSets are used to manage a set of Pods (even if the set consists of just a single Pod.) They accomplish this by using a very common pattern in Kubernetes called a reconciliation loop. The basic idea of a reconciliation loop is to input a desired state (e.g., the number of Pods we’d like to have running) and then constantly monitor or observe the current state of the k8s environment. If the reconciliation loop finds that desired == current, no action is taken, but if the opposite is true, it will create or destroy pods as needed until the the desired state is reached. Although, it’s important to understand that ReplicaSets an the Pods that they manage are loosely coupled. While a ReplicaSet may create or delete Pods, it does not own them. It manages them via a label selector defined in the ReplicaSet specification. This is also true of the Services that may load balance to the Pods - everything is very loosely coupled. This may seem daunting at first, but declarative configuration makes managing it much easier. There are also several distinct benefits to having these different k8s API objects being loosely coupled:\n- ReplicaSets may be deployed on top of pre-existing Pods and “adopt” them. If the ReplicaSet were tightly coupled to the Pods it managed, the Pods would need to be destroyed and recreated or a cutover would have to happen, possibly disrupting service.\n- When a Pod is misbehaving, you can alter it’s labels to effectively remove it from the ReplicaSet’s management. This allows the ReplicaSet to create a new, healthy Pod and the sick Pod is now quarantined. Developers can actively investigate the Pod in order to debug, rather than having only logs to sort through.\nSometimes you may wonder which ReplicaSet is managing a Pod. This kind of discovery is enabled by the ReplicaSet creating an annotation on every Pod it creates. The annotation key is kubernetes.io/created-by. This annotation is best-effort and only created when the Pod is created by the ReplicaSet - a user can remove the annotation at any time. If you are otherwise interesting in finding the Pods managed by a particular ReplicaSet, you can perform $ kubectl get pods -l &lt;key1=value1&gt;,&lt;key2=value2&gt;, where the labels are defined in the ReplicaSet spec. This is exactly the same API call that the ReplicaSet itself uses to find and manage its set of Pods.\nReplicaSets support imperative scaling via $ kubectl scale &lt;replica-set-name&gt; --replicas=4 for example. However, this is a stop gap measure to be sure. All declarative text-file configurations should be updated ASAP to reflect any imperative changes. This is to avoid a situation where the current state is dramatically different from a new, desired state being applied from a text-file configuration. Kubernetes also supports horizontal pod autoscaling (HPA). This is distinctly different from vertical pod autoscaling (giving Pods greater resource requests and limits) and cluster autoscaling (adding more worker nodes to the cluster). HPA requires the presence of the heapster Pod on your cluster, which included in most k8s installations by default and runs in the kube-system namespace. heapster keeps track of resource consumption metrics and provides an API to use when making autoscaling decisions. HPA is a separate k8s object from ReplicaSets and thus is loosely coupled. It is a bad idea to combine autoscaling with imperative or declarative management of the number of replicas. If you are using HPA, just manage the HPA object itself. Interfering with it could result in unexpected behavior.\nBy default, when you delete a ReplicaSet, you delete the Pods it is managing. You can avoid this by setting --cascade=false in your command - e.g., $ kubectl delete rs galens-rs --cascade=false."
  },
  {
    "objectID": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-5-pods",
    "href": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-5-pods",
    "title": "TL;DR - Kubernetes Up & Running, Part 2",
    "section": "",
    "text": "Pods represent a collection of application containers and volumes running in the same execution environment. Pods are the most atomic object within Kubernetes, therefore all containers within a Pod will always land on the same node within a cluster. Each container runs in its own cgroup and we will leverage that using requests and limits later on in this chapter. All applications running in a Pod share the same hostname, IP address and port space. Containers within a Pod can communicate with one another, but are isolated from containers in other Pods. Since a Pod can be composed of multiple containers, it’s natural to ask which containers should be included in any given Pod. The question to ask yourself is “Will these containers work correctly if they land on separate machines?” If the answer is “no”, then the containers should be placed in the same Pod. As we will see, it’s often the case that containers within the same Pod interact via a shared volume that is also within the Pod.\nPods are managed via YAML or JSON templates known as a Pod manifest. These manifests are text-file representations of the k8s API object. They are sent to kube-apiserver, which in turn passes them to the kube-scheduler. The scheduler then places the Pod onto a node that has sufficient resources. A daemon on the node, named kubelet, creates the containers declared in the Pod manifest and performs any health checks. Once a Pod as been scheduled to a node, there is no rescheduling if the node fails. The Kubernetes scheduler takes this into account and tries to ensure that Pods from the same application are distributed onto different nodes, in order to avoid a single failure domain. Once scheduled, will only change nodes if they are explicitly destroyed and rescheduled. When a Pod is deleted, it has a termination grace period with a default value of 30 seconds. This grace period allows the Pod to finish serving any active requests that it is processing before terminating. Any data stored within the containers on the Pod will be lost when the Pod terminates. The PersistentVolume object can be used to store data across multiple instances of Pod and their subsequent lifespans.\nOnce a Pod is running, you may want to access it even if it is not serving traffic on the Internet. You can use port forwarding to create a secure tunnel from your local machine, through the Kubernetes master, to the instance of the Pod running on a worker node.\nThere are several types of health checks for containers within Kubernetes. The first is a process health check. This health check simply checks if the main process of your application is running. If it isn’t, Kubernetes restarts it. This health check is automatic and does not need to be defined. A simple process check is often insufficient however. Imagine that your process is deadlocked - the health check will be green, but your application will be red. To address this, Kubernetes supports liveness health checks. Liveness health checks are defined within the Pod manifest per container and each container is health checked separately. They run application-specific logic to ensure that the application is not only running, but functioning properly. Lastly, there are readiness health checks. These checks are configured similarly to liveness probes. The difference is that containers which fail a liveness health check are restarted while containers that file a readiness probe are removed from Service load balancers. Combining liveness and readiness probes ensures that all traffic is routed to healthy containers that have capacity to fulfill the request. Kubernetes also supports tcpSocket health checks, which are useful for databases r other non-HTTP-based APIs. Finally, k8s supports exec probes. These execute a script in the context of a container - if the script returns zero, the probe succeeds. These are useful for applications where validation logic doesn’t fit neatly into an HTTP request.\nOne of the most powerful features of Pods is the ability to define the minimum and maximum compute resources available for the containers within the Pod. By rightsizing container requirements, kube-scheduler is able to efficiently pack Pods onto nodes, thereby driving up utilization. Requests specify the minimum resources needed by a Pod in order to be scheduled - limits specify the maximum resources a container may use. Both are declared at the container level within the Pod manifest. It’s worth mentioning that memory (unlike CPU) cannot be redistributed if it has already been allocated to a container process. Therefore, when the system runs out of memory, kubelet terminates containers whose memory usage is greater than their requested amount.\nVolumes are used for containers within a Pod to share some kind of state information. They can be used to communicate, synchronize, cache, or even persist state data beyond the lifespan of the Pod itself. Volumes are declared within the Pod manifest and container declarations may include volumeMounts. It’s important to note that containers can mount the same volume to different paths. emptyDir volumes can be used to create a shared filesystem between containers. hostDir volumes can grant access to the underlying host filesystem. There are multiple supported protocols for remote network storage volumes that can be used for truly persistent data, including NFS, iSCSI, and cloud network storage like AWS EBS, Azure FDS, and GCP Persistent Disk."
  },
  {
    "objectID": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-6-labels-and-annotations",
    "href": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-6-labels-and-annotations",
    "title": "TL;DR - Kubernetes Up & Running, Part 2",
    "section": "",
    "text": "Labels and annotations are types of metadata about Kubernetes API objects. They each serve a different purpose. Labels are used to identify and select sets of objects, while annotations are designed to hold nonidentifying information that can be leveraged by tools and libraries.\nKubernetes uses labels to group and select objects. There are a variety of ways to express the selection logic:\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\nkey=value\nkey is set to value\n\n\nkey!=value\nkey is not set to value\n\n\nkey in (value1, value2)\nkey is one of value1 or value2\n\n\nkey notin (value1, value2)\nkey is not one of value1 or value2\n\n\nkey\nkey is set\n\n\n!key\nkey is not set\n\n\n\nYou can use kubectl and labels to find and manage objects. Kubernetes itself also uses labels to accomplish the same thing.\nAnnotations are key-value pairs, much like labels, but they have less restrictions around string formatting. While this makes them exceedingly useful, it also means that there is no guarantee that the data contained in the annotation is valid. The nature/formatting of the data contained in the key-value pair can dictate whether it has to be an annotation or not. Outside of that, it’s a good practice to add information about an object as an annotation and promote it to a label if you find yourself wanting to use it in a selector.\nAnnotations can be used to:\n- Store comments about the latest change to an object.\n- Communicate a special scheduling policy to a specialized scheduler. - Build, release, and image information from source version control. - Prototype alpha functionality of the Kubernetes API. - Track status during rolling deployments and provide the necessary information needed to roll back to a previous version if required. - This is the primary use case for annotations."
  },
  {
    "objectID": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-7-service-discovery",
    "href": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-7-service-discovery",
    "title": "TL;DR - Kubernetes Up & Running, Part 2",
    "section": "",
    "text": "Service objects are used to expose Pods within a cluster and load-balance across them. Usually, a Service uses a named label selector to identify the Pods, but it is also possible to define a Service without selectors. By default, when a Service is created, k8s assigns it a virtual IP called a cluster IP. The cluster IP load-balances across the selected Pods. It also has a DNS name within kube-dns. In true k8s-building-on-k8s fashion, kube-dns is a Service with a ClusterIP. Because Service objects are responsible for load-balancing, they also handle the management of readiness checks. Besides ClusterIP, there are a few other ServiceTypes.\nNodePort builds upon ClusterIP and is used to serve traffic from outside of the cluster. In addition to a ClusterIP, kube-apiserver assigns a port to the Service (or you can specify the port yourself) and all nodes will forward traffic to that port to the Service. This means that if you can reach any node in the cluster, you can contact a service without knowing where that service is running. LoadBalancer builds upon NodePort. It integrates with cloud providers to provision a new load balancer and direct it at the nodes in your cluster, resulting in an IP or hostname depending on the cloud provider. Bonus material: there is also ExternalName if your cluster is running CoreDNS v1.7 or higher.\nFor every Service object, Kubernetes creates an Endpoints object which contains the IP addresses for that service. Kubernetes uses Endpoints to consume services rather than a cluster IP - which means your applications can too! By talking directly the k8s API, you can retrieve and call service endpoints. Kubernetes can also “watch” objects and be notified when they change, including the IPs associated with a service. Most applications are not written to be Kubernetes-native and therefore don’t leverage this, but it’s important to know that it’s possible.\nRegardless of the type, all Service objects will load-balance. This is made possible by a component named kube-proxy that runs on every node in the cluster. kube-proxy watches for new services via kube-apiserver. It programs a set of iptables rules (abstracted as ClusterIP) on in the kernel of the host, which change the destination of packets to one of the endpoints for that service. Whenever the set of endpoints changes, the set of iptables rules are rewritten. It is possible for a user to specify a specific cluster IP when creating a service. Once created, the cluster IP cannot be changed without deleting and rebuilding the Service object. In order to be assigned, a cluster IP must come from the cluster IP range defined by kube-apiserver and not already be in use. It’s possible to configure the service address range using the --service-cluster-ip-range flag on the kube-apiserver binary. The range should not overlap with the IP subnets and ranges assigned to each Docker bridge or Kubernetes node."
  },
  {
    "objectID": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-8-replicasets",
    "href": "posts/2019-09-09-kuar-part2/2019-09-09-kuar-part2.html#chapter-8-replicasets",
    "title": "TL;DR - Kubernetes Up & Running, Part 2",
    "section": "",
    "text": "ReplicaSets are used to manage a set of Pods (even if the set consists of just a single Pod.) They accomplish this by using a very common pattern in Kubernetes called a reconciliation loop. The basic idea of a reconciliation loop is to input a desired state (e.g., the number of Pods we’d like to have running) and then constantly monitor or observe the current state of the k8s environment. If the reconciliation loop finds that desired == current, no action is taken, but if the opposite is true, it will create or destroy pods as needed until the the desired state is reached. Although, it’s important to understand that ReplicaSets an the Pods that they manage are loosely coupled. While a ReplicaSet may create or delete Pods, it does not own them. It manages them via a label selector defined in the ReplicaSet specification. This is also true of the Services that may load balance to the Pods - everything is very loosely coupled. This may seem daunting at first, but declarative configuration makes managing it much easier. There are also several distinct benefits to having these different k8s API objects being loosely coupled:\n- ReplicaSets may be deployed on top of pre-existing Pods and “adopt” them. If the ReplicaSet were tightly coupled to the Pods it managed, the Pods would need to be destroyed and recreated or a cutover would have to happen, possibly disrupting service.\n- When a Pod is misbehaving, you can alter it’s labels to effectively remove it from the ReplicaSet’s management. This allows the ReplicaSet to create a new, healthy Pod and the sick Pod is now quarantined. Developers can actively investigate the Pod in order to debug, rather than having only logs to sort through.\nSometimes you may wonder which ReplicaSet is managing a Pod. This kind of discovery is enabled by the ReplicaSet creating an annotation on every Pod it creates. The annotation key is kubernetes.io/created-by. This annotation is best-effort and only created when the Pod is created by the ReplicaSet - a user can remove the annotation at any time. If you are otherwise interesting in finding the Pods managed by a particular ReplicaSet, you can perform $ kubectl get pods -l &lt;key1=value1&gt;,&lt;key2=value2&gt;, where the labels are defined in the ReplicaSet spec. This is exactly the same API call that the ReplicaSet itself uses to find and manage its set of Pods.\nReplicaSets support imperative scaling via $ kubectl scale &lt;replica-set-name&gt; --replicas=4 for example. However, this is a stop gap measure to be sure. All declarative text-file configurations should be updated ASAP to reflect any imperative changes. This is to avoid a situation where the current state is dramatically different from a new, desired state being applied from a text-file configuration. Kubernetes also supports horizontal pod autoscaling (HPA). This is distinctly different from vertical pod autoscaling (giving Pods greater resource requests and limits) and cluster autoscaling (adding more worker nodes to the cluster). HPA requires the presence of the heapster Pod on your cluster, which included in most k8s installations by default and runs in the kube-system namespace. heapster keeps track of resource consumption metrics and provides an API to use when making autoscaling decisions. HPA is a separate k8s object from ReplicaSets and thus is loosely coupled. It is a bad idea to combine autoscaling with imperative or declarative management of the number of replicas. If you are using HPA, just manage the HPA object itself. Interfering with it could result in unexpected behavior.\nBy default, when you delete a ReplicaSet, you delete the Pods it is managing. You can avoid this by setting --cascade=false in your command - e.g., $ kubectl delete rs galens-rs --cascade=false."
  },
  {
    "objectID": "posts/2015-04-11-fractal-interactive/2015-04-11-fractal-interactive.html",
    "href": "posts/2015-04-11-fractal-interactive/2015-04-11-fractal-interactive.html",
    "title": "Interactive Fractal Generator",
    "section": "",
    "text": "This is a Unity3d game that generates a box fractal. Once generated, there is a custom input map to an Xbox controller that allows for manipulation of the different “generations” of fractals.\nControls:\nL/R Triggers - Rotate generation selection\nL/R Bumpers - Set local rotation direction\nA - Translate generation away from center\nB - Translate generation towards center\nX - Reset generation position\nY - Reset generation rotation\nL/R Joysticks - Rotate generation around global axis\nView the repository here."
  },
  {
    "objectID": "posts/2016-08-01-isv3/2016-08-01-isv3.html",
    "href": "posts/2016-08-01-isv3/2016-08-01-isv3.html",
    "title": "Interactive Visualization of S3",
    "section": "",
    "text": "This application was developed in a Summer 2016 project at the Mathematical Computing Laboratory at UIC. It shows the hyperbolic space $S3 projected into 3 dimensions. This Unity build was then made VR capable for peripherals like the Oculus Rift.\nThis project was completed under the guidance and mentorship of Professor David Dumas. If you are interested in the mathematics of projective measured laminations, you can read about Professor Dumas’ work here.\nYou can view the repository here."
  },
  {
    "objectID": "posts/2019-08-12-gcp-developer/2019-08-12-gcp-developer.html",
    "href": "posts/2019-08-12-gcp-developer/2019-08-12-gcp-developer.html",
    "title": "GCP Professional Cloud Developer Certification",
    "section": "",
    "text": "Abstract\nThese are the raw format notes that I took while studying for the Google Cloud Professional Cloud Developer Certification. I’ve linked to the various materials that I used to study in the Resources section at the bottom. I passed the test and am officially certified!\n\n\nCode and Environment Management\n\nUse git or something similar\ndo not store jar file or other binaries in git\n\nbuild them on demand\n\ndo not store dependencies in git\n\ninstall them via a requirements file at build\nremember to use explicit versions!\n\ndon’t store configuration constants within the code itself\n\nuse a separate configuration file\nset them as environment varibles\n\n\nBenefits of microservice architecture compared to a monolithic application: - service boundaries roughly match business boundaries. e.g., payments are processed via the payments service, invoicing is done by the invoicing service. - services can be updated, deployed, and scaled independently of each other.\nIn order to achieve the promise of microservices, each service must be stateless. Being stateful causes availability issues if an instance of a service goes down and accessing a shared state is a bottleneck for scalability. Stateless service instances can start up quickly and shutdown gracefully.\n\n\nSecurity, Reliability, and Migration\n\nimplement healthchecks on storage, database, compute, network, etc\n\nyou can use this to route requests to unhealthy enpoints to a status page and return a 200.\n\ncollect, monitor, analyze logs. you can use log-based metrics to enable event triggering (e.g., autoscaling, notification, etc)\napplications must be be resilient to both transient and long-lasting errors when accessing data/resources in a distributed system.\n\nfor transient errors, use retry logic with exponential backoff\nfor long running errors, use flags/toggles to avoid/darken the service until it can be restored\n\ncalled “implementing a circuit breaker”\n\n\nperform high-availability/load testing and develop a disastery recovery plan\n\nthis is in addition to functional and performance testing\nexecute your disaster recovery plan as part of a scheduled “game day” by simulating failures\n\nexample failure scenarios include region/zone failure, deployment rollback, or network connectivity errors\n\n\nautomate wherever possible, working towards a DevSecOps methodology via CICD pipelines\nuse the strangler method to re-architect applications\n\nmigrate one component of a monolithic application at a time while leaving the original application fully functional\nrequests can be easily directed to the new or old appliction\nthis is a low risk migration pattern since it will not affect any business critical needs\n\n\n\n\nDatastore\n\nObjects are called entities.\nEntities are either root entities or have ancestors.\nEntities are composed of their key and properties\n\nkey is composed of namespace, entity kind, id, and ancestor id\nproperties can have one or more values\n\noperations on one or more entities are called transactions and are atomic\n\natomicity means that either all operations in a transaction are applied or none are.\n\nentities of the same kind do not need to have a consistent property set\nbuilt-in indexes\n\nautomatic index of each property of each kind\n\ncomposite indexes\n\nindex multiple property values\ndefined in an index configuration file\ntoo many will increase latency to achieve consistency\n\n\n3 types of queries:\n1) use keys-only when you only need the entity key\n2) use projection-query when you only need specific properties from the entity or properties included in the query filter\n3) use ancestor-query when you need strongly consistency\nDatastore is excellent for structured data that is non-relational. It scales extremely well. However, unlike relational databases, it does not support join operations. It does not support inequality filtering on multiple properties. It does not support filtering based on results of a subquery. Practically speaking, this means that you will often make two or more inequality queries and then compute the intersection.\nRemember that Datastore stores keys lexographically. High read/write activity to a local neighborhood of keys (or a relatively new key) will result in bottlenecks. For numeric keys, you can use the allocateIds() method to get keys that are distributed for performance. Outside of that, avoid negative numbers, the number zero, and monotomically increasing numbers.\nIf you need higher read capacity of a portion of the key range, you can use replication.\nDatastore transactions can fail when they run longer than 60 seconds, there are too many concurrent writes to the same entity group, or a transaction operates on more than 25 different entity groups. Datastore can return exceptions in cases where the transaction will eventually be committed succesfully - therefore, design your transactions to be idempotent. Idempotent means that a the final state will be the same even if a transaction is processed multiple times.\n\n\nMonitoring and Tuning Performance\nBe sure to measure and visualize The 4 Golden Metrics:\n1) Latency\n- Differentiate between successful and unsuccessful requests\n2) Traffic\n3) Errors\n4) Saturation\n\n\nResources:\n\n12 best practices for user account, authorization and password management\nChoosing an App Engine environment\nDeveloping Applications with Google Cloud Platform Specialization\nGoogle API Design Guide\nBig Table Schema Design\nBig Query Batch"
  },
  {
    "objectID": "posts/2020-09-15-how-to-win-friends/2020-09-15-how-to-win-friends.html",
    "href": "posts/2020-09-15-how-to-win-friends/2020-09-15-how-to-win-friends.html",
    "title": "How to Win Friends & Influence People",
    "section": "",
    "text": "Abstract\nI read a book! A pretty famous one at that. If you haven’t heard of it, you can Google it for yourself 🙃 This article is a super short blurb with the main bullet points from the book. I don’t think they will be very useful without the context that you gain by reading it, but I think having a quick reference guide can be a useful way to get a refresher before going into an important meeting or something along those lines.\n\n\nFundamental Techniques in Handling People\n\nDon’t criticize, condemn, or complain.\nGive honest and sincere appreciation.\nArouse in the other person an eager want.\n\n\n\nSix Ways to Make People Like You\n\nBecome genuinely interested in other people.\nSmile.\nRemember that a person’s name is to that person the sweetest and most important sound in any language.\nBe a good listener. Encourage others to talk about themselves.\nTalk in terms of the other person’s interests.\nMake the other person feel important - and do it sincerely.\n\n\n\nWin People to Your Way of Thinking\n\nThe only way to get the best of an argument is to avoid it.\nShow respect for the other person’s opinions. Never say, “You’re wrong.”\nIf you are wrong, admit it quickly and emphatically.\nBegin in a friendly way.\nGet the other person saying “yes, yes” immediately.\nLet the other person do a great deal of the talking.\n\n\n\nBe a Leader\n\nBegin with praise and honest appreciation.\nCall attention to people’s mistakes indirectly.\nTalk about your own mistakes before criticizing the other person.\nAsk questions instead of giving direct orders.\nLet the other person save face.\nPraise the slightest improvement and praise every improvement. Be “hearty in your approbation and lavish in your praise”.\nGive the other person a fine reputation to live up to.\nUse encouragement. Make the fault seem easy to correct.\nMake the other person happy about doing the thing you suggest."
  },
  {
    "objectID": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html",
    "href": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html",
    "title": "TL;DR - Kubernetes Up & Running, Part 1",
    "section": "",
    "text": "These are my notes from reading Kubernetes Up & Running by Kelsey Hightower, Brendan Burns, and Joe Beda. Kelsey Hightower is a Staff Developer Advocate for the Google Cloud Platform. Brendan Burns is a Distinguished Engineer in Microsoft Azure and cofounded the Kubernetes project at Google. Joe Beda is the CTO of Heptio and cofounded the Kubernetes project, as well as Google Compute Engine.\nThis is a phenomenal book that covers both the whys and hows of Kubernetes. I read the 1st edition, but a 2nd edition is coming out soon. I’m using this as study material for my CKAD and CKA certifications.\nBonus material: Kubernetes is commonly stylized as k8s. This is because the first letter of Kubernetes is k, the last letter is s, and there are 8 letters in-between the first and last.\n\n\nKubernetes delivers 4 main benefits: velocity, scaling (of both software and teams), abstraction of infrastructure, and efficiency.\nVelocity is achieved using 3 core concepts: immutability, declarative configuration, and online self-healing systems. Immutability is the practice of replacing the current image with a brand new one, rather than updating it incrementally. The advantage of this approach is that there is a record of the delta between the images that can be used to troubleshoot any errors. Furthermore, you can rollback to the previous image if the new one doesn’t work - this is much more difficult if you are applying incremental changes to the same image. This ties directly into the concept of declarative configuration. Kubernetes itself manages changes to its state in a declarative fashion - each new desired state is declared concretely. Changes are not implied imperatively. Finally, Kubernetes manages this declared state continuously. If the actual state is different from the desired state, k8s will take action to create the desired state. All of these concepts add up to less time spent on operations and debugging, and more time developing new features.\nKubernetes achieves scaling for both applications and teams via decoupled architecture. By isolating all components of a distributed system via APIs and load balancers, each system may scale independently. APIs result in a crisp surface area between components. Teams can stay relatively small (and therefore agile) and have ownership of their microservice without having to coordinate with other teams - everyone can refer to the API schema. Additionally, clusters are able to add and remove machines very easily. This allows for low lead times on additional compute substrate, allowing applications to scale as needed, but it also means that a small operations team can support many clusters and many more development teams. This idea of fungible machines within a cluster ties directly into abstracted infrastructure.\nBecause k8s treats all machines as fungible, it allows for portability of Kubernetes APIs between IaaS providers (i.e., public cloud) and data centers. There are specific “gotchas” if you are using vendor managed services (e.g., AWS DynamoDB), but for the most part, Kubernetes is able to abstract components like load balancers and storage within different clouds. Bonus material: although not in this book, GCP Anthos takes this idea one step further. Anthos provides a single pane of glass to manage a Kubernetes cluster or clusters that can span multiple clouds as well as on-prem. This gives companies a highly abstracted viewpoint for application development and operations. It also gives the business a clear road map for cloud migration, multi-cloud HA/DR capabilities, and more.\nLastly, k8s achieves efficiency by colocating applications on the same machine without impacting the applications themselves. This means that more work can be done by fewer machines. This results in direct economic benefits, but also allows for new development methodologies. Rather than scaling testing environments at the VM level, a single cluster can support the entire testing environment. This makes it possible to test every single commit or pull request, every time, throughout your entire stack. This efficiency and thoroughness feeds directly back into developer productivity and velocity.\n\n\n\nThe Docker image format for containers is the de facto standard. It is composed of a series of root filesystem layers, one on top of another. Each layer adds, removes, or modifies the preceding layer in the filesystem. In practice, it looks something like this:\n\nContainer A: base operating system only\n\nContainer B: build upon #A, by adding Ruby v2.1.10\n\nContainer D: build upon #B by adding Rails v4.2.6\n\nContainer C: build upon #A, by adding Golang v1.6\n\n\nThe layering of these filesystems can result in extensive directed acyclic graphs that encompass a multitude of container images. It is worth noting that there is a counter-intuitive problem regarding the layering of filesystems. Deleting a file in a newer layer does not remove it from the preceding layers. This means that passwords/secrets should never be baked into images because they will still exist and be accessible to anyone with the right tools. Piggybacking on this idea, it’s important to order your layers according to which layers are most likely to change; these layers should be “at the top” or the newest of all layers. In the example above, if Container A were to change often, Containers B, C, and D would each need to be re-built to incorporate the changes. Beyond this, it is a best practice to keep your application containers as lean as possible. The smaller the container binary, the more efficiently they can be allocated on compute.\nDocker provides the ability to limit the amount of resources used by a container by exposing the underlying cgroup technology provided by the Linux kernel. Flags like --memory, --memory-swap, and --cpu-shares accomplish this. Setting these restrictions is important to ensure that colocated applications have fair and predictable access to compute resources.\nWhenever you build a new Docker image, it remains on your computer until explicitly deleted - even if you create a new image with the exact same tag/name. Use $ docker images to list what is currently on your computer and remove what you’re no longer using. Bonus material: The book lists a deprecated garbage collector as a tool to stay on top of this. You can use $ docker system prune instead. Set this up as a cron job to be extra clean (and fancy).\n\n\n\nAt this point, the book describes multiple ways to get a Kubernetes cluster up and running (yuck yuck yuck) so that we can begin to interact with the k8s API. Most of these options are via public cloud. The book references services for both Azure and Amazon Web Services that have since been replaced by Kubernetes-as-a-Service offerings, similar to Google Kubernetes Engine (GKE). I’ve provided links to the new services. It is also worth noting that some of the commands listed in the book have changed as services have evolved since the time of publishing. Reference the documentation for your chosen service/tool to get your k8s cluster up and running.\n\nGCP GKE\nAKS\nEKS\nminikube (run k8s locally)\n\nkubectl is the official Kubernetes CLI tool and provides all of the functionality needed to interact with your clusters APIs. It is distinctly different from kubeadm. Using commands like $ kubectl describe nodes, you can see detailed information about all of the components of your cluster and what is running on them. The components that comprise Kubernetes are actually deployed using Kubernetes itself. These core components are located in the kube-system namespace, where a namespace is an entity for organizing and isolating Kubernetes resources (kind of like a folder in a filesystem.)\nkube-proxy is responsible for routing network traffic to load-balanced services within the cluster. It has to be running on every node in order to function properly. Kubernetes also runs a DNS server, which provides naming and discovery for all of the services defined in the cluster. The DNS server will run several duplicates of itself depending on the size of the cluster. These replicas are managed by a kube-dns deployment and there is a separate kube-dns Service that handles load balancing for the DNS server.\nFinally, there is a Kubernetes dashboard UI. Similar to kube-dns, there is a Deployment (this time only a single replica) to manage reliability for the dashboard as well as a Service to manage load balancing. Both run under the name kubernetes-dashboard. Bonus material: The dashboard is not deployed by default. You can deploy it using $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml. Most public cloud KaaS offerings will remove the need for the dashboard by offering similar information about your cluster through the web console.\n\n\n\nThe following are kubectl commands that apply to all Kubernetes objects.\nNamespaces\nBy default, kubectl interacts with objects in the default namespace. If you need access to a different namespace (like kube-system) you need to pass the --namespace flag.\nContexts\nYou can use contexts to rewrite default configurations. For example, you can the default namespace or even the k8s cluster that you are managing. Contexts are usually stored in a configuration file at $HOME/.kube/config. By defining and using different contexts, you can save yourself from having to explicitly type a lot of parameters and values.\nViewing Kubernetes API Objects\nEverything within Kubernetes is defined by a RESTful schema. Each k8s object exists at a unique HTTP path (e.g., https://galens-cluster/api/v1/namespaces/default/pods/galens-pod) and kubectl works by sending HTTP requests to those endpoints. In RESTful fashion, the most basic command for viewing objects is get. You can use this to get all of a resource type or a specific object. You can add the -o wide flag to have the output include more information on longer lines, as well as -o json and -o yaml. A particularly useful flag is --no-headers. This skips the header at the beginning of a human-readable table and makes it very easy to pipe the output into other Unix commands. It is also very common to need to retrieve a specific field from the object. kubectl uses the JSONPath query language to select fields from the returned object. For example, $ kubectl get pods galens-pod -o jsonpath --template={.status.podIP} would return the IP address of the Pod. Finally, $ kubectl describe &lt;resource-name&gt; &lt;obj-name&gt; provides rich, multiline, human-readable information about an object, as well as any other related objects and events.\nCreating, Updating, and Destroying Kubernetes Objects\nMost of the time, k8s objects are created from a template file (YAML or JSON) using $ kubectl apply -f galens-object.yaml. Similarly, any changes to the object’s template are applied using the same command. You can use $ kubectl edit &lt;resource-name&gt; &lt;obj-name&gt; to modify objects inplace. You can use $ kubectl delete -f galens-object.yaml or $ kubectl delete &lt;resource-name&gt; &lt;obj-name&gt;, but Kubernetes will not ask for confirmation before deleting the resource - be careful!\nLabeling and Annotating Objects\nLabels and annotations are key-value pairs that can be used in various ways within Kubernetes. They can be applied using $ kubectl label and $ kubectl annotate respectively. Kubernetes will not allow you to overwrite an existing label by default. To do that, you need to use the --overwrite flag.\nDebugging Commands You can get the logs from a pod by running $ kubectl logs &lt;pod-name&gt; and add the -c flag to specify a specific container if there are multiple in the pod. By default, logs will return the current logs and exit. You can use -f(follow) to continuously stream the container logs. You are also able to get an active shell within a container using $ kubectl exex -it &lt;pod-name&gt; -- bash. Lastly, use $ kubectl cp &lt;pod-name&gt;:/path/to/remote/file /path/to/local/file to copy files back and forth between your local machine and the container.\nHelp As always - --help is here to make everything possible!"
  },
  {
    "objectID": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-1-introduction",
    "href": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-1-introduction",
    "title": "TL;DR - Kubernetes Up & Running, Part 1",
    "section": "",
    "text": "Kubernetes delivers 4 main benefits: velocity, scaling (of both software and teams), abstraction of infrastructure, and efficiency.\nVelocity is achieved using 3 core concepts: immutability, declarative configuration, and online self-healing systems. Immutability is the practice of replacing the current image with a brand new one, rather than updating it incrementally. The advantage of this approach is that there is a record of the delta between the images that can be used to troubleshoot any errors. Furthermore, you can rollback to the previous image if the new one doesn’t work - this is much more difficult if you are applying incremental changes to the same image. This ties directly into the concept of declarative configuration. Kubernetes itself manages changes to its state in a declarative fashion - each new desired state is declared concretely. Changes are not implied imperatively. Finally, Kubernetes manages this declared state continuously. If the actual state is different from the desired state, k8s will take action to create the desired state. All of these concepts add up to less time spent on operations and debugging, and more time developing new features.\nKubernetes achieves scaling for both applications and teams via decoupled architecture. By isolating all components of a distributed system via APIs and load balancers, each system may scale independently. APIs result in a crisp surface area between components. Teams can stay relatively small (and therefore agile) and have ownership of their microservice without having to coordinate with other teams - everyone can refer to the API schema. Additionally, clusters are able to add and remove machines very easily. This allows for low lead times on additional compute substrate, allowing applications to scale as needed, but it also means that a small operations team can support many clusters and many more development teams. This idea of fungible machines within a cluster ties directly into abstracted infrastructure.\nBecause k8s treats all machines as fungible, it allows for portability of Kubernetes APIs between IaaS providers (i.e., public cloud) and data centers. There are specific “gotchas” if you are using vendor managed services (e.g., AWS DynamoDB), but for the most part, Kubernetes is able to abstract components like load balancers and storage within different clouds. Bonus material: although not in this book, GCP Anthos takes this idea one step further. Anthos provides a single pane of glass to manage a Kubernetes cluster or clusters that can span multiple clouds as well as on-prem. This gives companies a highly abstracted viewpoint for application development and operations. It also gives the business a clear road map for cloud migration, multi-cloud HA/DR capabilities, and more.\nLastly, k8s achieves efficiency by colocating applications on the same machine without impacting the applications themselves. This means that more work can be done by fewer machines. This results in direct economic benefits, but also allows for new development methodologies. Rather than scaling testing environments at the VM level, a single cluster can support the entire testing environment. This makes it possible to test every single commit or pull request, every time, throughout your entire stack. This efficiency and thoroughness feeds directly back into developer productivity and velocity."
  },
  {
    "objectID": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-2-creating-and-running-containers",
    "href": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-2-creating-and-running-containers",
    "title": "TL;DR - Kubernetes Up & Running, Part 1",
    "section": "",
    "text": "The Docker image format for containers is the de facto standard. It is composed of a series of root filesystem layers, one on top of another. Each layer adds, removes, or modifies the preceding layer in the filesystem. In practice, it looks something like this:\n\nContainer A: base operating system only\n\nContainer B: build upon #A, by adding Ruby v2.1.10\n\nContainer D: build upon #B by adding Rails v4.2.6\n\nContainer C: build upon #A, by adding Golang v1.6\n\n\nThe layering of these filesystems can result in extensive directed acyclic graphs that encompass a multitude of container images. It is worth noting that there is a counter-intuitive problem regarding the layering of filesystems. Deleting a file in a newer layer does not remove it from the preceding layers. This means that passwords/secrets should never be baked into images because they will still exist and be accessible to anyone with the right tools. Piggybacking on this idea, it’s important to order your layers according to which layers are most likely to change; these layers should be “at the top” or the newest of all layers. In the example above, if Container A were to change often, Containers B, C, and D would each need to be re-built to incorporate the changes. Beyond this, it is a best practice to keep your application containers as lean as possible. The smaller the container binary, the more efficiently they can be allocated on compute.\nDocker provides the ability to limit the amount of resources used by a container by exposing the underlying cgroup technology provided by the Linux kernel. Flags like --memory, --memory-swap, and --cpu-shares accomplish this. Setting these restrictions is important to ensure that colocated applications have fair and predictable access to compute resources.\nWhenever you build a new Docker image, it remains on your computer until explicitly deleted - even if you create a new image with the exact same tag/name. Use $ docker images to list what is currently on your computer and remove what you’re no longer using. Bonus material: The book lists a deprecated garbage collector as a tool to stay on top of this. You can use $ docker system prune instead. Set this up as a cron job to be extra clean (and fancy)."
  },
  {
    "objectID": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-3-deploying-a-kubernetes-cluster",
    "href": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-3-deploying-a-kubernetes-cluster",
    "title": "TL;DR - Kubernetes Up & Running, Part 1",
    "section": "",
    "text": "At this point, the book describes multiple ways to get a Kubernetes cluster up and running (yuck yuck yuck) so that we can begin to interact with the k8s API. Most of these options are via public cloud. The book references services for both Azure and Amazon Web Services that have since been replaced by Kubernetes-as-a-Service offerings, similar to Google Kubernetes Engine (GKE). I’ve provided links to the new services. It is also worth noting that some of the commands listed in the book have changed as services have evolved since the time of publishing. Reference the documentation for your chosen service/tool to get your k8s cluster up and running.\n\nGCP GKE\nAKS\nEKS\nminikube (run k8s locally)\n\nkubectl is the official Kubernetes CLI tool and provides all of the functionality needed to interact with your clusters APIs. It is distinctly different from kubeadm. Using commands like $ kubectl describe nodes, you can see detailed information about all of the components of your cluster and what is running on them. The components that comprise Kubernetes are actually deployed using Kubernetes itself. These core components are located in the kube-system namespace, where a namespace is an entity for organizing and isolating Kubernetes resources (kind of like a folder in a filesystem.)\nkube-proxy is responsible for routing network traffic to load-balanced services within the cluster. It has to be running on every node in order to function properly. Kubernetes also runs a DNS server, which provides naming and discovery for all of the services defined in the cluster. The DNS server will run several duplicates of itself depending on the size of the cluster. These replicas are managed by a kube-dns deployment and there is a separate kube-dns Service that handles load balancing for the DNS server.\nFinally, there is a Kubernetes dashboard UI. Similar to kube-dns, there is a Deployment (this time only a single replica) to manage reliability for the dashboard as well as a Service to manage load balancing. Both run under the name kubernetes-dashboard. Bonus material: The dashboard is not deployed by default. You can deploy it using $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml. Most public cloud KaaS offerings will remove the need for the dashboard by offering similar information about your cluster through the web console."
  },
  {
    "objectID": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-4-common-kubectl-commands",
    "href": "posts/2019-09-08-kuar-part1/2019-09-08-kuar-part1.html#chapter-4-common-kubectl-commands",
    "title": "TL;DR - Kubernetes Up & Running, Part 1",
    "section": "",
    "text": "The following are kubectl commands that apply to all Kubernetes objects.\nNamespaces\nBy default, kubectl interacts with objects in the default namespace. If you need access to a different namespace (like kube-system) you need to pass the --namespace flag.\nContexts\nYou can use contexts to rewrite default configurations. For example, you can the default namespace or even the k8s cluster that you are managing. Contexts are usually stored in a configuration file at $HOME/.kube/config. By defining and using different contexts, you can save yourself from having to explicitly type a lot of parameters and values.\nViewing Kubernetes API Objects\nEverything within Kubernetes is defined by a RESTful schema. Each k8s object exists at a unique HTTP path (e.g., https://galens-cluster/api/v1/namespaces/default/pods/galens-pod) and kubectl works by sending HTTP requests to those endpoints. In RESTful fashion, the most basic command for viewing objects is get. You can use this to get all of a resource type or a specific object. You can add the -o wide flag to have the output include more information on longer lines, as well as -o json and -o yaml. A particularly useful flag is --no-headers. This skips the header at the beginning of a human-readable table and makes it very easy to pipe the output into other Unix commands. It is also very common to need to retrieve a specific field from the object. kubectl uses the JSONPath query language to select fields from the returned object. For example, $ kubectl get pods galens-pod -o jsonpath --template={.status.podIP} would return the IP address of the Pod. Finally, $ kubectl describe &lt;resource-name&gt; &lt;obj-name&gt; provides rich, multiline, human-readable information about an object, as well as any other related objects and events.\nCreating, Updating, and Destroying Kubernetes Objects\nMost of the time, k8s objects are created from a template file (YAML or JSON) using $ kubectl apply -f galens-object.yaml. Similarly, any changes to the object’s template are applied using the same command. You can use $ kubectl edit &lt;resource-name&gt; &lt;obj-name&gt; to modify objects inplace. You can use $ kubectl delete -f galens-object.yaml or $ kubectl delete &lt;resource-name&gt; &lt;obj-name&gt;, but Kubernetes will not ask for confirmation before deleting the resource - be careful!\nLabeling and Annotating Objects\nLabels and annotations are key-value pairs that can be used in various ways within Kubernetes. They can be applied using $ kubectl label and $ kubectl annotate respectively. Kubernetes will not allow you to overwrite an existing label by default. To do that, you need to use the --overwrite flag.\nDebugging Commands You can get the logs from a pod by running $ kubectl logs &lt;pod-name&gt; and add the -c flag to specify a specific container if there are multiple in the pod. By default, logs will return the current logs and exit. You can use -f(follow) to continuously stream the container logs. You are also able to get an active shell within a container using $ kubectl exex -it &lt;pod-name&gt; -- bash. Lastly, use $ kubectl cp &lt;pod-name&gt;:/path/to/remote/file /path/to/local/file to copy files back and forth between your local machine and the container.\nHelp As always - --help is here to make everything possible!"
  },
  {
    "objectID": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html",
    "href": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html",
    "title": "TL;DR - Kubernetes Up & Running, Part 3",
    "section": "",
    "text": "These are my notes from reading Kubernetes Up & Running by Kelsey Hightower, Brendan Burns, and Joe Beda. Kelsey Hightower is a Staff Developer Advocate for the Google Cloud Platform. Brendan Burns is a Distinguished Engineer in Microsoft Azure and cofounded the Kubernetes project at Google. Joe Beda is the CTO of Heptio and cofounded the Kubernetes project, as well as Google Compute Engine.\nThis is a phenomenal book that covers both the whys and hows of Kubernetes. I read the 1st edition, but a 2nd edition is coming out soon. I’m using this as study material for my CKAD and CKA certifications.\nThis article is part of a series. You can read Part 1 and Part 2\n\n\nThe Pods deployed by a ReplicaSet are completely decoupled from the node they are running on - the pods can run anywhere and/or multiple Pods can be on the same node. The DaemonSet is distinctly different in that it it places a Pod onto every node in the cluster (or a subset of nodes). The Pods managed by a DaemonSet are usually landing some sort of agent or daemon onto the node. They are not traditional serving applications (like ReplicaSet Pods), but instead augment the capabilities of the cluster itself. By defining DaemonSets in declarative configuration, we can be sure that Pods are running on all of the proper nodes, even in an autoscaling cluster where nodes from and go freely.\nWhich nodes a DaemonSet runs on in a cluster is defined in the DaemonSet spec using labels. It’s possible to select a subset of nodes. Common use cases for this are selecting nodes with certain hardware (e.g., GPUs or SSDs). In order to do this, nodes must be properly labeled. Here is an example command to label a node: $ kubectl label nodes galens-awesome-node-123 gpu=true. This label can now be specified in the NodeSelector field of the DaemonSet spec. Because DaemonSets manage Pods using a reconciliation loop, if any required labels are removed from a node, the DaemonSet Pods will also be removed. Similar to a ReplicaSet, if a DaemonSet is deleted, its Pods will be deleted as well unless you are using --cascade=false.\nPrior to Kubernetes version 1.6, updating DaemonSets required updating the declarative configuration for the the DaemonSet and then performing a rolling delete of each Pod, or deleting the entire DaemonSet and redeploying. While the latter is much simpler, the drawback is downtime. A rolling delete/update can be performed using the following code snippet:\nPODS=$(kubectl get pods -o jsonpath -template='{.items[*]metadata.name}')\nfor x in $PODS; do\n  kubectl delete pods ${x}\n  sleep 60 #delete one pod every 60 seconds\ndone\nThe delete method is still the default update strategy for DaemonSets in order to support backwards compatibility. However, newer versions of Kubernetes now support a rolling update strategy similar to Deployments. You will need to configure spec.updateStrategy.type field of the DaemonSet to have the value RollingUpdate. Any changes to the DaemonSet spec.template field or subfields will trigger a rolling update. Rolling updates come with two additional parameters: - spec.minReadySeconds, determines how long a Pod’s status must be “ready” before moving onto the next Pod\n- spec.updateStrategy.rollingUpdate.maxUnavailable, how many Pods can be being updated simultaneously\nIt’s best practice to set spec.minReadySeconds to something like 30-60 seconds to ensure that Pods are truly healthy before proceeding. Setting spec.updateStrategy.rollingUpdate.maxUnavailable to 1 is the safest value, but depending on the size of the application and cluster, can result in long rollouts. Increasing the value increases the blast radius for a failed rollout. It’s best practice to set the value low and only increase it if users or admins complain about rollout speed.\n\n\n\nJobs are used to run short-lived, one-off tasks. They create and manage Pods that run until successful termination (i.e., exit with 0). If a Pod fails before successful termination, the Job controller will create another one from the Pod template in the Job spec. When a Job completes, the Job and related Pod are still around. You will need to provide the -a flag in kubectl to see completed Jobs. Jobs can be created both imperatively and declaratively. Both options will use the parameter/field restartPolicy. It is recommended to use restartPolicy=OnFailure so that Pods are recreated in place. Using restartPolicy=Never will create an entirely new Pod after each failure and can lead to a lot of “junk”. It’s not uncommon for a bug to cause a Pod to crash as soon as it starts. This behavior is monitored by kubelet on the node and will set the Pod status to CrashLoopBackOff without the Job controller doing anything. CrashLoopBackOff delays the Pod from being recreated to avoid eating resources on the node. Pods can also appear healthy, but be deadlocked. Jobs support liveness probes to determine Pod health in these situations.\nJobs have 2 major parameters that control their behavior, completions and parallelism. parallelism determines how many copies of a Pods to spin up at once. completions determines the number of successful exits before a Job stops running. If completions is left unset, the Job will be put into a worker pool mode. Once the first Pod exits successfully, the Job will start winding down and not add any new Pods. This means that none of the workers should exit until the work is done and they are all in the process of finishing up.\n\n\n\nWe want to make our container images as reusable and portable as possible. In order to do this, we need to be able to configure them at runtime so that the application runs properly according to its environment. This is where ConfigMaps and Secrets come in handy. In essence, both ConfigMaps and Secrets provide key-value pairs to containers right before they are run.\nThere are 3 main ways to use a ConfigMap: 1. Filesystem: the ConfigMap is mounted as a volume in the container. A file is created for each key-pair based on the key name. The contents of the file are set to the value. 2. Environment variable: Set an environment variable $KEY=VALUE. 3. Command-line argument: Reference environment variables in command-line.\nBecause key names for both ConfigMaps and Secrets are designed to be able to map to valid environment variable names, they have appropriate naming constraints. If a ConfigMap or Secret is updated, the new information becomes available to the application without restarting. However, this means your application must be written to reread its configuration values. Currently, there is no built-in way to signal an application when a new version of a ConfigMap or Secret is deployed. ConfigMap values are UTF-8 text, where as Secret values can hold arbitrary data encoded in base64, which makes it possible to store binary data. However, this makes it much more difficult to manage secrets stored in YAML files.\nSecrets can be consumed via the k8s API, or more preferably via a secrets volume. Secrets volumes are managed by the kubelet and store secrets on tmpfs volumes - the secret is never written to disk on nodes. There is a special use case for secrets to access private Docker registries that is supported via image pull secrets. These are consumed just like regular secrets, but are declared in the spec.imagePullSecret field of the Pod manifest."
  },
  {
    "objectID": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html#chapter-9-daemonsets",
    "href": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html#chapter-9-daemonsets",
    "title": "TL;DR - Kubernetes Up & Running, Part 3",
    "section": "",
    "text": "The Pods deployed by a ReplicaSet are completely decoupled from the node they are running on - the pods can run anywhere and/or multiple Pods can be on the same node. The DaemonSet is distinctly different in that it it places a Pod onto every node in the cluster (or a subset of nodes). The Pods managed by a DaemonSet are usually landing some sort of agent or daemon onto the node. They are not traditional serving applications (like ReplicaSet Pods), but instead augment the capabilities of the cluster itself. By defining DaemonSets in declarative configuration, we can be sure that Pods are running on all of the proper nodes, even in an autoscaling cluster where nodes from and go freely.\nWhich nodes a DaemonSet runs on in a cluster is defined in the DaemonSet spec using labels. It’s possible to select a subset of nodes. Common use cases for this are selecting nodes with certain hardware (e.g., GPUs or SSDs). In order to do this, nodes must be properly labeled. Here is an example command to label a node: $ kubectl label nodes galens-awesome-node-123 gpu=true. This label can now be specified in the NodeSelector field of the DaemonSet spec. Because DaemonSets manage Pods using a reconciliation loop, if any required labels are removed from a node, the DaemonSet Pods will also be removed. Similar to a ReplicaSet, if a DaemonSet is deleted, its Pods will be deleted as well unless you are using --cascade=false.\nPrior to Kubernetes version 1.6, updating DaemonSets required updating the declarative configuration for the the DaemonSet and then performing a rolling delete of each Pod, or deleting the entire DaemonSet and redeploying. While the latter is much simpler, the drawback is downtime. A rolling delete/update can be performed using the following code snippet:\nPODS=$(kubectl get pods -o jsonpath -template='{.items[*]metadata.name}')\nfor x in $PODS; do\n  kubectl delete pods ${x}\n  sleep 60 #delete one pod every 60 seconds\ndone\nThe delete method is still the default update strategy for DaemonSets in order to support backwards compatibility. However, newer versions of Kubernetes now support a rolling update strategy similar to Deployments. You will need to configure spec.updateStrategy.type field of the DaemonSet to have the value RollingUpdate. Any changes to the DaemonSet spec.template field or subfields will trigger a rolling update. Rolling updates come with two additional parameters: - spec.minReadySeconds, determines how long a Pod’s status must be “ready” before moving onto the next Pod\n- spec.updateStrategy.rollingUpdate.maxUnavailable, how many Pods can be being updated simultaneously\nIt’s best practice to set spec.minReadySeconds to something like 30-60 seconds to ensure that Pods are truly healthy before proceeding. Setting spec.updateStrategy.rollingUpdate.maxUnavailable to 1 is the safest value, but depending on the size of the application and cluster, can result in long rollouts. Increasing the value increases the blast radius for a failed rollout. It’s best practice to set the value low and only increase it if users or admins complain about rollout speed."
  },
  {
    "objectID": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html#chapter-10-jobs",
    "href": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html#chapter-10-jobs",
    "title": "TL;DR - Kubernetes Up & Running, Part 3",
    "section": "",
    "text": "Jobs are used to run short-lived, one-off tasks. They create and manage Pods that run until successful termination (i.e., exit with 0). If a Pod fails before successful termination, the Job controller will create another one from the Pod template in the Job spec. When a Job completes, the Job and related Pod are still around. You will need to provide the -a flag in kubectl to see completed Jobs. Jobs can be created both imperatively and declaratively. Both options will use the parameter/field restartPolicy. It is recommended to use restartPolicy=OnFailure so that Pods are recreated in place. Using restartPolicy=Never will create an entirely new Pod after each failure and can lead to a lot of “junk”. It’s not uncommon for a bug to cause a Pod to crash as soon as it starts. This behavior is monitored by kubelet on the node and will set the Pod status to CrashLoopBackOff without the Job controller doing anything. CrashLoopBackOff delays the Pod from being recreated to avoid eating resources on the node. Pods can also appear healthy, but be deadlocked. Jobs support liveness probes to determine Pod health in these situations.\nJobs have 2 major parameters that control their behavior, completions and parallelism. parallelism determines how many copies of a Pods to spin up at once. completions determines the number of successful exits before a Job stops running. If completions is left unset, the Job will be put into a worker pool mode. Once the first Pod exits successfully, the Job will start winding down and not add any new Pods. This means that none of the workers should exit until the work is done and they are all in the process of finishing up."
  },
  {
    "objectID": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html#chapter-11-configmaps-and-secrets",
    "href": "posts/2019-09-12-kuar-part3/2019-09-12-kuar-part3.html#chapter-11-configmaps-and-secrets",
    "title": "TL;DR - Kubernetes Up & Running, Part 3",
    "section": "",
    "text": "We want to make our container images as reusable and portable as possible. In order to do this, we need to be able to configure them at runtime so that the application runs properly according to its environment. This is where ConfigMaps and Secrets come in handy. In essence, both ConfigMaps and Secrets provide key-value pairs to containers right before they are run.\nThere are 3 main ways to use a ConfigMap: 1. Filesystem: the ConfigMap is mounted as a volume in the container. A file is created for each key-pair based on the key name. The contents of the file are set to the value. 2. Environment variable: Set an environment variable $KEY=VALUE. 3. Command-line argument: Reference environment variables in command-line.\nBecause key names for both ConfigMaps and Secrets are designed to be able to map to valid environment variable names, they have appropriate naming constraints. If a ConfigMap or Secret is updated, the new information becomes available to the application without restarting. However, this means your application must be written to reread its configuration values. Currently, there is no built-in way to signal an application when a new version of a ConfigMap or Secret is deployed. ConfigMap values are UTF-8 text, where as Secret values can hold arbitrary data encoded in base64, which makes it possible to store binary data. However, this makes it much more difficult to manage secrets stored in YAML files.\nSecrets can be consumed via the k8s API, or more preferably via a secrets volume. Secrets volumes are managed by the kubelet and store secrets on tmpfs volumes - the secret is never written to disk on nodes. There is a special use case for secrets to access private Docker registries that is supported via image pull secrets. These are consumed just like regular secrets, but are declared in the spec.imagePullSecret field of the Pod manifest."
  },
  {
    "objectID": "posts/2019-08-23-how-and-why-capitalism-needs-to-be-reformed/2019-08-23-how-and-why-capitalism-needs-to-be-reformed.html",
    "href": "posts/2019-08-23-how-and-why-capitalism-needs-to-be-reformed/2019-08-23-how-and-why-capitalism-needs-to-be-reformed.html",
    "title": "Why and How Captialism Needs to be Reformed by Ray Dalio",
    "section": "",
    "text": "Abstract\nRay Dalio, the founder of Bridgewater Associates, published a two-part article on April 5, 2019 titled Why and How Capitalism Needs to be Reformed. The first section is about Ray’s perspective both as an American and further as the founder and CEO (although not currently) of one of the world’s largest and most successful hedge funds. The second part of the article offers an explaination why capitalism is failing and what he thinks we should and can do about it.\nThis piece is already written in a succinct fashion with lots of bullet points and summaries. This TL;DR attempts to distill this distillation even further. I highly recommend reading the original article, which is available here.\n\n\nTL;DR\n\nThere is income/education/wealth/opportunity gap in America and it is broadening. This is a self-reinforcing, generational problem.\nHistorically, this is a phenomenally bad sign for society. In the past, if there is an economic downturn, it has been followed by conflict.\nThe coordination of monetary and fiscal policy, coulpled with leadership from the top, can help to shift money and credit to those who need it more and will spent it (i.e., boost the economy.)\n\n\n\nPart I\n\nThere has been little or no inflation adjusted income growth for the lower 60% of American household incomes since 1980.\n\nIn the same time, incomes for the top 10% have doubled. Incomes for the top 1% have tripled.\n\nThe wealth gap is the highest since the late 1930s.\n\nThe top 1% of the world has more wealth than the bottom 90%.\n\nAmerican workers in the lowest earning quartile have one of the lowest probabilities of moving into a higher quartile in the entire world.\nThere is a income/education/wealth/opportunity gap that has been rampantly growing.\nThe current American system is largely failing children. Many children in the USA are poor, physically and mentally malnourished, and poorly educated.\n\nOverall, American scores poorly in testing compared to the rest of the world. These scores can be correlated with poverty rates.\n\nThe income/education/wealth/opportunity gap reinforces itself.\n\nOver 1/3 of public school funding comes from real estate tax. Wealther neighborhoods have better schools. Wealthier students have better outcomes which turn into higher productivity and earnings which come back to their neighborhood. Their neighborhood becomes even wealthier, taxes bring in more money, the schools become better, the expected outcomes of students improve.\nThe exact opposite is true for poor neighborhoods.\n\n\n“To me, leaving so many children in poverty and not educating them well is the equivalent of child abuse, and it is economically stupid.”\n-Ray Dalio\n“These gaps weaken us economically because:\nThey slow our economic growth because the marginal propensity to spend of wealthy people is much less than the marginal propensity to spend of people who are short of money. They result in suboptimal talent development and lead to a large percentage of the population undertaking damaging activities rather than contributing activities.”\n-Ray Dalio\nThis income/education/wealth/opportunity gap breeds populism on both the left and right, which is not condusive to a stable economy or political system. In the event of an economic downturn, history shows that there is an extremely high probability of conflict and revolution. Conflict and revolution are not good for growing the economic pie. But history also shows that they also fall short of their promise to redistribute the pie as well.\nSo in summary:\nINVEST IN CHILDREN.\nALL CHILDREN.\nFEED THEM.\nEDUCATE THEM.\nENSURE THEY HAVE SAFE HOMES AND STABLE GUARDIANS.\n\n\nPart II\nHow to fix capitalism in America:\n\nLeadership from the top.\nBipartisan commission tasked with simultaneously dividing and increasing the economic pie.\n\nFocused on “double bottom line investments” - good social returns and good economic returns.\n\nClear metrics that can be used to judge success and hold the people in charge accountable for achieving it.\nRedistribution of resources that will improve both the well-beings and the productivities of the vast majority of people.\n\nCreate public-private partnerships with clear metrics to evaluate success.\nTax things that create an over-all burden on the economy (e.g., pollution)\nTax the top more and earmark the money specifically for the middle and bottom. The savings produced by investing in the middle and bottom will yield more over-all economic gains than the taxes taken.\n\nEstablish minimum universal standards for healthcare and education."
  },
  {
    "objectID": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html",
    "href": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html",
    "title": "How I Built This Website",
    "section": "",
    "text": "Up until about a week ago, I had a pretty sweet website that I hosted using GitHub Pages. When my student account expired, I decided not to buy the GitHub Pro membership that would let me continue to host my website. Instead, I decided to put my knowledge of AWS to good use and host it there instead. I’ve set up everything from my own SSL/TLS certificate to a CI/CD pipeline. I got a kick out of building it. I’m looking forward to continuing to experiment and build the website out. Here’s how I did it:"
  },
  {
    "objectID": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#infrastructure",
    "href": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#infrastructure",
    "title": "How I Built This Website",
    "section": "Infrastructure",
    "text": "Infrastructure\nThe first thing you’re going to need for your brand spankin’ new website is a domain name. I went to Google Domains to grab mine. They have a good selection of endings, fair prices, and it’s a super easy process. There are lots of other places you can purchase domain names - they will all work with the website infrastucture we’re going to set up.\nOnce you’ve got your domain name (e.g., galenballew.fyi), it’s time to get our hands dirty in the cloud. I built my website, CDN, and CICD pipeline all in AWS, but Azure and GCP have the same functionality.\n\nS3 Buckets\n\nCreate a bucket for server access logs.\n\nI like to set up a Lifecycle Management Policy to delete logs after 90 days.\n\nCreate a bucket named galenballew.fyi\n\nEnable this bucket for static website hosting.\nBesure to uncheck the box that says “Prevent public policies”.\nApply a public bucket policy or ACL.\nConfigure logging to the logging bucket\n\nCreate a bucket named www.galenballew.fyi\n\nUnder Static Website Hosting, configure the bucket to redirect requests to galenballew.fyi via https.\nConfigure logging to the logging bucket.\n\n\n\n\nContent Distribution Network\nA CDN is not necessary for your website, but it does bring some value. Namely, CloudFront will cache objects out on it’s edge locations. This will speed your website up for anyone accessing it from another part of the country or world. Further, your CloudFront distribution will allow end users to access it over HTTPS - security first folks. It’s worth noting that CloudFront distributions come with a lot of configurablity. I’m only going to cover the settings that were important for my simple website.\n\nGo to AWS Certificate Manger and request a new public certificate.\n\nProvide the root domain as the domain name (i.e., galenballew.fyi).\nGive www.galenballew.fyi as an additional name.\nI opted for DNS validation - it will take care of itself in Route53.\n\nGo to CloudFront and create a new distribution (web delivery).\n\nSet logs to your logging bucket.\nSet HTTP to be redirected to HTTPS.\nSet the Price Class to US, Canada, and Europe (use your best judgement here, I wanted to save a few pennies.)\nProvide galenballew.fyi and www.galenballew.fyi as CNAMEs.\nSet SSL to the certificate you created in step 1.\nWhen setting the origin, be sure to use the actual website endpoint (e.g.,galenballew.fyi.s3-website-us-east-1.amazonaws.com) not the bucket itself. If you’re setting this up via the web console, there will be a drop down menu that allows you to select the S3 bucket as the origin. This is not the same as setting the origin to the website endpoint!\nDeploy that sucker. It takes a few minutes to deploy. You can use the time to set up DNS.\n\n\n\n\nDomain Name System\n\nGo to AWS Route 53 and create a new hosted zone.\n\nName the zone after the root domain.\nInside the hosted zone, create an alias record for the root.\n\nSelect and A - IPv4 record type and Alias: Yes. Provide the CloudFront distribution domain name as the Alias Target.\n\nCreate another record set for www.galenballew.fyi the exact same way.\n\nCopy the 4 name server domain names in the hosted zone. These are the 4 values listed in the NS record set for the root domain of your website.\n\nGo to Google Domains, or whatever provider you chose, and find the DNS settings for your domain. There should be an option to use custom name servers.\nPop the 4 domain names you wrote down into as the name servers for your domain."
  },
  {
    "objectID": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#static-site-content",
    "href": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#static-site-content",
    "title": "How I Built This Website",
    "section": "Static Site & Content",
    "text": "Static Site & Content\nAlright, we have a website, but there is nothing there! Not even an index.html root document. We need to put some content up for people to see. I looked at a few different static site generators, including Hugo and Jekyll. Ultimately, I decided on Jekyll after trying out some of the Hugo themes. I even went so far as to pay for the theme I’m using because I liked it so much - Jekyll Journal Theme.\nI will leave it as an exercise to the reader to get Jekyll installed and configured. The docs are a good place to start. Once you have a directory with some static site content in it, I would highly recommend storing your website in version control. If you are using Perforce or Mercurial all the power to you, but for everyone else in the world, go with Git. If you have a GitHub account, set up a repository and push your content to it. This is great for managing your content, but it will also become critical when setting up a CI/CD pipeline.\nOne other thing I like to do is add the _site/ directory to my .gitignore file. It’s not necessary to save in version control since it will be repopulated with each bundle exec jekyll build."
  },
  {
    "objectID": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#cicd",
    "href": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#cicd",
    "title": "How I Built This Website",
    "section": "CI/CD",
    "text": "CI/CD\nThis last bit is, in my opinion, the best part. AWS makes it super easy to integrate with GitHub for continuous integration and continuous deployment/delivery. Everything that happens in the build process is contained in a YAML configuration file inside of your website directory. It makes publishing and fix-forward updates painless.\n\nGo to AWS CodePipeline and create a new pipeline.\n\nI used default values whenever possible.\nIntegrate the Source to be your GitHub account (or perhaps CodeCommit if you’re fancy!)\nI set mine to use GitHub webhooks. This mean’s the pipeline will execute after any commits are pushed to the master branch.\n\nSelect new CodeBuild project.\n\nKeep things simple here. I went with the smallest Linux environment available.\nSelect Use a buildspec file. I went with the default name and location.\nEnable CloudWatch logs so you can monitor the logs for your build in real-time. This can be super useful if you need to debug your buildspec.yml\nEnable S3 logs as well if you’re into it.\n\nCreate the pipeline!\n\nNow that the pipeline is up, it will execute every time the GitHub webhooks fire off a POST. But what exactly will execute? Whatever is in your buildspec.yml. You’ll want to place this file at the root of your website directory. Here is my buildspec.yml:\nversion: 0.2\nphases:\n install:\n   commands:\n     - echo \"install step\"\n     - gem update --system --quiet\n     - gem install jekyll jekyll-paginate jekyll-sitemap jekyll-gist\n     - echo \"jekyll installed\"\n     - gem uninstall bundler\n     - gem install bundler\n     - bundle install\n     - echo \"bundle installed\"\n build:\n   commands:\n     - echo \"building step\"\n     - bundle exec jekyll build --verbose\n     - echo \"building complete\"\n post_build:\n   commands:\n     - echo \"post_build step\"\n     - aws s3 sync --delete _site/ s3://galenballew.fyi/\n     - echo \"files transferred successfully\"\n     - aws cloudfront create-invalidation --distribution-id E1BBD9XD38EPQC --paths \"/*\"\n     - echo \"CDN cache invalidated\"\nJust to highlight the major events: 1. Jekyll and bundler are installed 2. The website is built 3. The _site/ directory is synced to the root S3 bucket. --delete ensures that anything in the bucket that is not in _site/ will be deleted from the bucket. 4. Everything cached in CloudFront is invalidated.\nThis is a simple and heavy handed buildspec, but it works great! Tinker with it as needed."
  },
  {
    "objectID": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#conclusion",
    "href": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#conclusion",
    "title": "How I Built This Website",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a really fun project to set up. My only regret is that I did not do it via CloudFormation so that I could share it with all of you lovely people, or easily dupliate my infrastructure in new regions. It’s probably going to cost me a few more dollars to host my website in AWS than to pay $7/month for GitHub Pages, but I think it’s worth it. At least my website won’t go down if I make it to the front page of Hacker News :smile:"
  },
  {
    "objectID": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#resources",
    "href": "posts/2019-02-24-how-i-built-this-website/2019-02-24-how-i-built-this-website.html#resources",
    "title": "How I Built This Website",
    "section": "Resources",
    "text": "Resources\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteAccessPermissionsReqd.html\nhttps://jekyllrb.com/docs/"
  },
  {
    "objectID": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html",
    "href": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html",
    "title": "How I Landed My Dream Job Working On Self-Driving Cars",
    "section": "",
    "text": "May 12, 2017 - This article was published by Udacity on their Medium blog.\nI graduated high school in 2009 and I knew back then that the Tesla Roaster was cool — really cool. I paid attention to Tesla Motors and followed the release of the Model S. During college I would go to the nearest Telsa showroom and go for test rides just for fun. Now, I had no idea what the future held. I was studying mathematics and art at the time. I got into deep learning (DL) through style transfer and was hooked. It was only after I graduated that I began to learn about all of the applications DL was about to revolutionize — especially autonomous vehicles."
  },
  {
    "objectID": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#first-things-first",
    "href": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#first-things-first",
    "title": "How I Landed My Dream Job Working On Self-Driving Cars",
    "section": "First Things First",
    "text": "First Things First\nLike a lot of young people, I hadn’t the foggiest idea what I was going to do when I graduated last fall. Mathematics is a wonderful thing, but it’s not very career specific. Just a few months after graduating, I made two very important decisions: to enroll at Metis and to enroll in the Udacity Self-driving Car Engineer Nanodegree (SDCEND). Both of these were instrumental in my career path, but the Udacity SDCEND was critical.\nWhen I first signed up, I wasn’t expecting to end up working in the self-drive car industry. All I knew was that I was genuinely interested in the content being taught and that it would be applicable in multiple domains. Later on, Udacity released Nanodegrees in Robotics and Artificial Intelligence. There is a lot of overlap in all three of these programs. Eventually, I’d like to take all three of them."
  },
  {
    "objectID": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#the-self-driving-car-nanodegree",
    "href": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#the-self-driving-car-nanodegree",
    "title": "How I Landed My Dream Job Working On Self-Driving Cars",
    "section": "The Self-Driving Car Nanodegree",
    "text": "The Self-Driving Car Nanodegree\nThe Udacity SDCEND constists of 3 terms, each term lasting 12 weeks. You can read about the course syllabus for each term here. The entire Nanodegree touches on all aspects of the autonomous vehicle pipeline by having students build out multiple projects per term. The goal is to help students be well-rounded and understand the fundamentals of the big picture, as well as develop an impressive GitHub presence (and a blog too if you do the career support extracurriculars!) For me, it means being exposed to a lot of engineering (which is new to me) and tying it into machine learning (my passion).\nUdacity partnered with several corporate sponsors for both content creation and as hiring partners — names like Mercedes-Benz, BMW, and NVIDIA. Currently, I’m watching videos made by engineers at Mercedes-Benz and I’ve applied to McLaren Applied Technologies through the private hiring portal. To me, this is amazing and it shows how serious Udacity is taking the emergence of automation revolution. Before I was a student, I didn’t take Udacity’s open-source self-driving car very seriously. Now, I want to become a contributor (more on this soon!) You can check out their GitHub repository for their self-driving car here.\nI’m not the only student who has become more committed to the field through the program. The private Slack channel for students is filled with a tangible excitement. I’ve never been a part of a such a large student body, let alone a student body that is committed to the success of every student (no grading curve here). Between Slack, the dedicated forums, and your own private mentor, there is no reason to be stuck on a problem — there are so many people willing to help answer your questions. Instead, you can focus on finding your own way to improve the foundations of the projects."
  },
  {
    "objectID": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#what-to-expect",
    "href": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#what-to-expect",
    "title": "How I Landed My Dream Job Working On Self-Driving Cars",
    "section": "What to Expect",
    "text": "What to Expect\nThe course took me about 10 hours per week. This can vary a lot depending on your level of experience with coding, machine learning, engineering etc. If you’re anything like me, this means 0 hours per week until the project is close to due and then working feverishly all weekend. I managed to complete Term 1 while attending an immersive data science bootcamp and I’m doing Term 2 while working full-time.\nwarning — I’m about to get on a soapbox If all you want is to pass the program and add a certification to your LinkedIn, the program expects very little of you. As the program grows, there are more examples available, and more answers documented. If you want to work on self-driving cars, you need to take ownership of the curriculum. You need to do research outside of the lectures. You need to incorporate the examples and answers that are available and then build on them. The program is not designed to make you a principal engineer. It’s designed to help you get your foot in the door — to give you a platform to demonstrate your work ethic and passion. This is exactly what got me my job."
  },
  {
    "objectID": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#my-story",
    "href": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#my-story",
    "title": "How I Landed My Dream Job Working On Self-Driving Cars",
    "section": "My story",
    "text": "My story\nI finished Term 1 of the Udacity SDCEND about the same time I graduated from Metis. I spent several weeks utilizing every job search trick in the book and sent out close to 50 applications for various data science positions. I interviewed with several companies leading up to graduation and in the weeks that followed.\nUltimately, I received an email from a principal engineer at HERE Technologies asking if I would like to interview with him about an internship opportunity on his Highly Automated Driving team.\nI chose HERE for a few reasons:\n\nThe principal research engineer who made me the offer was an outstanding person. I was more than happy to work under him.\nThe work and commitment I demonstrated by transitioning from pure mathematics to self-driving car technology was recognized and rewarded.\nI am working with experts in computer vision and deep learning. I knew this position would set me up to continue learning.\nThe job is in Boulder, CO. I was happy for the change of pace and to hit the slopes!\nI’m part of the Highly Automated Driving team. Our platform and models are going to impact the world in a very real way, very soon.\n\nHERE is primarily a mapping company (HERE WeGo). They have an established track record of success since the 80’s and have been working in the automotive space since 2015 when they were purchased by Volkswagen, BMW, and Daimler — later Tencent and Intel would also purchase a stake in the company."
  },
  {
    "objectID": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#there-is-no-spoon",
    "href": "posts/2017-05-12-how-i-got-my-job/2017-05-12-how-i-got-my-job.html#there-is-no-spoon",
    "title": "How I Landed My Dream Job Working On Self-Driving Cars",
    "section": "There Is No Spoon",
    "text": "There Is No Spoon\nTechnology is an amazing industry to work in. Often it’s the case that merit and ability are rewarded as much or more than seniority. If you have access to the internet, you can become proficient at a seemingly endless number of tech-related skills. If there is something you want to work on, you don’t need to wait to get started. Dive in, build a portfolio, and demonstrate interest.\nIf you complete a program like the Udacity SDCEND, be open when opportunity knocks. I accepted the internship with HERE despite that I am doing DevOps work instead of building sexy convolutional neural networks. The self-driving car pipeline is massive. There are so many different specialties coming together to build something remarkable. Whether it’s what you work on or what kind of work you do, find out what’s important to you and pursue it."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "",
    "text": "May 11, 2017 - This article was published by Towards Data Science on their Medium blog.\nI graduated with my undergraduate in mathematics last August. I had done some programming and school and I loved it, but I wasn’t competitive with computer science or engineering students. I wanted to take the next step and program as a career, but I needed to do it quickly before my student loans kicked in. I started looking into bootcamps to jump-start my career. Tech bootcamps are everywhere. They’re in every major city and they’re for all different roles in the technology sector. Most of them are for front-end web development, but I chose to pursue data science. In April, I graduated from Metis after 12 weeks of intensive data science training. I had to put in a significant chunk of cash, as well as a lot of hard work, but I got a lot back in return."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#what-can-a-bootcamp-do-for-you",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#what-can-a-bootcamp-do-for-you",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "What can a bootcamp do for you?",
    "text": "What can a bootcamp do for you?\nRight now, most bootcamps are for front-end web development, but there are a variety of different types. The open-floor office space that Metis’ Chicago campus operates in is shared with Dev Bootcamp. I got to learn the ins and outs of more than one kind of bootcamp while at Metis. For the sake of this article, I’m going to speak strictly about my personal experience, but many bootcamps share common themes or structure because the people who run them look at other, successful bootcamps for examples.\nThe point of bootcamps is to get you a job. This does not mean that you are going to become a master at your craft in 12 weeks. In fact, when you graduate is when the real learning begins. During the bootcamp you will find out exactly how much you don’t know and from there you can start the long journey of continuing to learn, skill up, and practice. Bootcamps exist to get you up to speed quickly, give you as much hands-on, practical experience as they can cram in, and help you land that job afterwards."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#a-day-in-bootcamp",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#a-day-in-bootcamp",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "A day in bootcamp",
    "text": "A day in bootcamp\nSo what exactly does Metis’ data science bootcamp look like? For starters, it’s 12 weeks long, 5 days a week, 9 hours per day. That doesn’t include the time you work outside of class. Bootcamps will use the word immersive — this is exactly what they mean. During the 12 weeks, I built 5 machine learning projects that used real-world data, I also created a blog to showcase some of them, and made sure my GitHub was up to snuff. On top of that, I practiced coding, problem solving, and presentation skills with my classmates every single day. Bootcamps revolve around the idea of lecturing on a topic and then immediately putting it to action. There is so much to learn and so little time, you need to get your hands dirty in order for things to stick.\nOutside of the classroom, I attended at least one event every week. These could be guest speakers, networking events, or just socializing. Many data scientists came to talk strictly with my class, tell us about their work, and answers questions. Other people came for bigger events that were open to the public — Irmak Sirer even gave a talk on using Generative Adversarial Networks to create images of his face! All the events were a great opportunity to network and also to unwind after class.\nI received some form of career training every week. This came in the form of one-on-one meetings with my career adviser, different workshops, or new resources shared with us to use in preparation for our career search. Not to mention the professional head-shots (just look at my LinkedIn), business cards, and serious resume makeovers. We also spent a lot of time doing mock interviews (technical and otherwise) which proved to be invaluable. At the end, we hosted a career night event that was slammed with recruiters and I had a chance to present a passion project I had spent three weeks working on."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#post-graduation",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#post-graduation",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "Post-Graduation",
    "text": "Post-Graduation\nOnce you finish a bootcamp, you enter a community of alumni. For Metis (and Dev Bootcamp), this meant a private alumni Slack channel. At least once a day there is a new job posting. Usually it’s by someone one the careers staff, but more senior alumni often post that their company is looking to hire — talk about a great network! People also post all kinds of great articles and resources. It’s a great way to network and meet up with people if you’re attending something like PyCon or another conference. The best part is that you’re a member for life. Looking down the road, you’ll have a network of hundreds of data scientists (developers etc.) with various amounts of experience. You’ll have direct access to opportunities that will help advance your career, as well as the chance to hire smart, driven people who just graduated.\nAdditionally, Metis and Dev Bootcamp graduates are granted access to their private hiring platform Employ. Employ is kind of like LinkedIn, but only for Metis and Dev Bootcamp graduates (both camps are owned by the same parent company in case you’re wondering) and for their hiring partners. This means recruiters or data scientists can view your profile and highlighted projects. Likewise, you can reach out to someone at a prospective company directly and get a response!\nTo top it off, you’re granted continued access to the office space if you want to come in and work. This means you’ll get more face-time with your career adviser — who will always be your career adviser — and you can continue attending the weekly events."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#my-experience",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#my-experience",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "My experience",
    "text": "My experience\nIt’s time for the details you’ve all been waiting for! I’ll keep this to the need-to-know. If there are any questions you’d like answered just leave a comment on the story and I’ll get back to you. Before Metis, I was interviewing for positions in the $50–62k range. After graduation, I am competitive in a range of $85–105k. The bootcamp cost $15,500. I studied pure mathematics in college and did some programming. As much as I love abstract mathematics, I really love machine learning. I could not be more excited about my career, my newfound passion, and the future.\nI interviewed with several companies leading up to graduation and in the weeks that followed. Ultimately, I accepted a paid internship with the possibility of FTE afterwards at HERE Technologies. I chose HERE for a few reasons: * The principal research engineer who made me the offer was an outstanding person. I was more than happy to work under him.\n* I am working with experts in computer vision and deep learning. I knew this position would set me up to continue learning. * The job was in Boulder, CO. I was happy for the change of pace and to hit the slopes! * I’m part of the Highly Automated Driving team. Our platform and models are going to impact the world in a very real way, very soon.\nFor me, this summed up to a dream job. There was no way I could have gone from a bachelors in Mathematics to such a niche and competitive position without the additional training I received at Metis."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#what-did-i-forget-to-mention",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#what-did-i-forget-to-mention",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "What Did I Forget to Mention…?",
    "text": "What Did I Forget to Mention…?\nThe people. When you’re thinking about signing up for a bootcamp, it’s a really big decision and a personal one. It’s all about you, your career, your money, and your life. Almost all bootcamps have an application process and they end up turning away a lot of applicants. What this means is that you will be surrounded by other students who all made the same big decision and all deserve to be there. Your fellow students will be amazing and you will always share your time in bootcamp together! I had no idea that those relationships would end up being the most valuable part of the bootcamp experience.\nLikewise, the instructors are phenomenal. They come from different backgrounds, but are all experts with a lot of experience. I learned so much from them in 12 weeks and a lot of it was knowledge that wasn’t on the curriculum. I learned career tips, more niche machine learning techniques, and much more. I count my instructors among my friends and some of my best contacts in my network."
  },
  {
    "objectID": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#disclaimer",
    "href": "posts/2017-05-10-are-bootcamps-worth-it/2017-05-10-are-bootcamps-worth-it.html#disclaimer",
    "title": "Are Tech Bootcamps Worth It?",
    "section": "Disclaimer",
    "text": "Disclaimer\nObviously this is my personal experience, yours may not be the same. Be sure to do your homework, be selective, and most of all be ready to work. Attending a bootcamp does not entitle you to a job. If you attend one, you will inevitably hear a horror story about someone who didn’t complete it. Just remember what bootcamps exist for — to get you up to speed quickly, give you as much hands-on, practical experience as they can cram in, and help you land that job afterwards. The rest is up to you."
  },
  {
    "objectID": "posts/2015-10-27-visap/2015-10-27-visap.html",
    "href": "posts/2015-10-27-visap/2015-10-27-visap.html",
    "title": "VisAP 2015",
    "section": "",
    "text": "All video and editing done by Galen Ballew."
  },
  {
    "objectID": "posts/2020-09-14-terraform/2020-09-14-terraform.html",
    "href": "posts/2020-09-14-terraform/2020-09-14-terraform.html",
    "title": "Terraform for Dummies",
    "section": "",
    "text": "Table of Contents\n\nAbstract\nCLI Tips and Tricks\nProviders\nTerraform State\nTerraform Settings\nProvisioners Are a Last Resort\nMaster the Workflow\nLearn More Subcommands\n\nTainting\n\nUse and Create Modules\nLifecycle Blocks\nLocal-only Resources\nOperation Timeouts\nData Sources\nResources\n\n\n\nAbstract\nWe are building a platform at work that needs to support Terraform infrastructure-as-code. I decided to get ramped up on the technology so that I wouldn’t be talking out of my ass about it. As part of getting ramped up, I found out that Hashicorp offers a certification for Terraform, so I decided to pursue it.\nThis article is basically a rehash of the Terraform Study Guide. The notes here are basically bullet points that are written from my perspective (lots of experience with IaC and cloud in general). There’s also an egregious amount of copy pasta.\nThese notes are in no way comprehensive. They’re just what I felt I needed to sit the certification exam and feel comfortable talking about Terraform at work. Here are some things that I don’t really get into: Terraform Cloud, modules, module registries, resource dependencies, expressions, operators, complex types, functions, and dynamic blocks. I would check these out in the documentation if you don’t know what they are.\nLastly, after passing the certification, I would recommend spending a little extra time studying everything about variables. This article is light on the subject and I wish I was a little more knowledgable about them.\n\n\nCLI Tips and Tricks\nEnable tab completion. If you use either bash or zsh, you can enable tab completion for Terraform commands. To enable autocomplete, run the following commands.\n$ terraform -install-autocomplete\n$ exec bash\nI also set up $ alias tf=\"terraform\" (similar to k=\"kubectl\" for all my Kuberhomies out there.)\n\n\nProviders\nTerraform includes the meta-argument alias for configuring a provider multiple ways for different resources. You can select which provider configuration to use on a per-resource or per-module basis. The primary reason for this is to support multiple regions for a cloud platform; other examples include targeting multiple Docker hosts, multiple Consul hosts, etc.\n\n\nTerraform State\nState locking is default behavior if your backend supports it. You can manually override the lock, but it’s dangerous to do so.\nWhen Terraform is used to manage larger systems, teams should use multiple separate Terraform configurations that correspond with suitable architectural boundaries within the system so that different components can be managed separately and, if appropriate, by distinct teams. Workspaces alone are not a suitable tool for system decomposition, because each subsystem should have its own separate configuration and backend, and will thus have its own distinct set of workspaces.\nYou can use backends that support remote state to create an operating model for infrastructure consumption across teams:\nFor example, a core infrastructure team can handle building the core machines, networking, etc. and can expose some information to other teams to run their own infrastructure. As a more specific example with AWS: you can expose things such as VPC IDs, subnets, NAT instance IDs, etc. through remote state and have other Terraform states consume that.\nTerraform state can contain sensitive data, depending on the resources in use and your definition of “sensitive.” The state contains resource IDs and all resource attributes. For resources such as databases, this may contain initial passwords.\nIf you manage any sensitive data with Terraform (like database passwords, user passwords, or private keys), treat the state itself as sensitive data.\n\n\nTerraform Settings\nBeyond tinkering with the CLI, you can alter Terraform settings within a configuration file, like so:\nterraform {\n   # ...\n}\nWithin a terraform block, only constant values can be used; arguments may not refer to named objects such as resources, input variables, etc, and may not use any of the Terraform language built-in functions.\nYou can use this block to configure things like: - Backend configuration - Specifying a version of Terraform - Specifying Provider Requirements - Opt-in to experimental language features - Passing metadata to Providers\n\n\nProvisioners Are a Last Resort\nDirect quote from the docs: Provisioners can be used to model specific actions on the local machine or on a remote machine in order to prepare servers or other infrastructure objects for service.\nTerraform includes the concept of provisioners as a measure of pragmatism, knowing that there will always be certain behaviors that can’t be directly represented in Terraform’s declarative model.\nHowever, they also add a considerable amount of complexity and uncertainty to Terraform usage. Firstly, Terraform cannot model the actions of provisioners as part of a plan because they can in principle take any action. Secondly, successful use of provisioners requires coordinating many more details than Terraform usage usually requires: direct network access to your servers, issuing Terraform credentials to log in, making sure that all of the necessary external software is installed, etc.\nYou can also set up destroy-time provisioners using a conditional when statement:\nresource \"aws_instance\" \"web\" {\n  # ...\n\n  provisioner \"local-exec\" {\n    when    = destroy\n    command = \"echo 'Destroy-time provisioner'\"\n  }\n}\n\n\nMaster the Workflow\nThe Core Terraform Workflow has three steps: 1. Write 2. Plan 3. Apply\nAs teams and the infrastructure grows, so does the number of sensitive input variables (e.g. API Keys, SSL Cert Pairs) required to run a plan. It’s best practice to have a robust Continuous Integration pipeline to make the iterative process easier and more secure. Not only does this make writing IaC better, but it helps to review pull requests because a CI pipeline is able to automatically include the output of tf plan along with the PR.\n\n\nLearn More Subcommands\nYou are able to import existing resources into the Terraform state. Currently, Terraform is not able to generate a configuration for the resource when importing, so you will need to author the resource configuration yourself. Be sure that each resource you import is mapped to only one Terraform resource address.\nThe terraform fmt command is used to rewrite Terraform configuration files to a canonical format and style. Tack on the -diff flag to display all formatting changes:\n$ tf fmt -diff\nThe terraform validate command validates the configuration files in a directory, referring only to the configuration and not accessing any remote services such as remote state, provider APIs, etc.\n$ tf validate\nValidate is safe to run automatically. Running terraform plan also performs a validation, but pulls in the contextual information about a run, like the target workspace, input variables, etc.\nThe terraform show command is used to provide human-readable output from a state or plan file. This can be used to inspect a plan to ensure that the planned operations are expected, or to inspect the current state as Terraform sees it. You can also generate machine-readable output by using the -json flag.\nThe terraform refresh command is used to reconcile the state Terraform knows about (via its state file) with the real-world infrastructure. This can be used to detect any drift from the last-known state, and to update the state file.\n\nTainting\nDirect quote from Get Started - AWS on HashiCorp Learn:\nIf a resource successfully creates but fails during provisioning, Terraform will error and mark the resource as “tainted”. A resource that is tainted has been physically created, but can’t be considered safe to use since provisioning failed.\nWhen you generate your next execution plan, Terraform will not attempt to restart provisioning on the same resource because it isn’t guaranteed to be safe. Instead, Terraform will remove any tainted resources and create new resources, attempting to provision them again after creation.\nTerraform also does not automatically roll back and destroy the resource during the apply when the failure happens, because that would go against the execution plan: the execution plan would’ve said a resource will be created, but does not say it will ever be deleted. If you create an execution plan with a tainted resource, however, the plan will clearly state that the resource will be destroyed because it is tainted.\nYou can also manually taint resources using $ tf taint resource.id\n\n\n\nUse and Create Modules\nUse the version attribute in the module block to specifcy verions:\nmodule \"consul\" {\n  source  = \"hashicorp/consul/aws\"\n  version = \"0.0.5\"\n\n  servers = 3\n}\n\n\nLifecycle Blocks\nWithin a resource block, you can detail special lifecycle behavior for resources using the nested lifecycle block. The following meta-arguments are available to all resources, regardless of type.\n\ncreate_before_destroy: Creates a new resource before deleting the existing one when set to true. Used when updates cannot be applied in-place.\nprevent_destroy: When set to true, will cause Terraform to reject with an error any plan that would destroy the infrastructure object associated with the resource, as long as the argument remains present in the configuration.\nignore_changes: Accepts a list of attribute names as parameters. This attributes are ignored when applying future configuration updates. This is useful for the (rare) circumstances where a system outside of Terraform changes the value of an attribute and it should not be rolled back by Terraform.\n\n\n\nLocal-only Resources\nWhile most resource types correspond to an infrastructure object type that is managed via a remote network API, there are certain specialized resource types that operate only within Terraform itself, calculating some results and saving those results in the state for future use.\nFor example, local-only resource types exist for generating private keys, issuing self-signed TLS certificates, and even generating random ids. While these resource types often have a more marginal purpose than those managing “real” infrastructure objects, they can be useful as glue to help connect together other resources.\nThe behavior of local-only resources is the same as all other resources, but their result data exists only within the Terraform state. “Destroying” such a resource means only to remove it from the state, discarding its data.\n\n\nOperation Timeouts\nCertain resource types have nested timeout block arguments available to them. These blocks allow you to customize how long certain operations are allowed to take before being considered to have failed.\n\n\nData Sources\nData sources allow Terraform to access information that is outside of the configuration file itself. The data is retrieved using a data block.\ndata \"aws_ami\" \"example\" {\n  most_recent = true\n\n  owners = [\"self\"]\n  tags = {\n    Name   = \"app-server\"\n    Tested = \"true\"\n  }\n}\n\n\nResources\n\nTerraform Study Guide\nTerraform Tutorials\nTerraform Documentation\nTerraform GitHub"
  },
  {
    "objectID": "posts/2017-02-17-opencv-lane-detection/2017-02-17-opencv-lane-detection.html",
    "href": "posts/2017-02-17-opencv-lane-detection/2017-02-17-opencv-lane-detection.html",
    "title": "OpenCV for Lane Detection in Self Driving Cars",
    "section": "",
    "text": "I have this article pushlished over on Medium. Click here to read it."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html",
    "title": "The 6 Rs of Cloud Migration",
    "section": "",
    "text": "This post is a quick summary of 6 common migration strategies that Amazon observes from its customers. You can read the original post here. It’s part of a series of posts about cloud migration."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#rehosting",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#rehosting",
    "title": "The 6 Rs of Cloud Migration",
    "section": "1. Rehosting",
    "text": "1. Rehosting\n\nAKA “lift-and-shift”\nRehosting does not take advantage of any cloud-native capabilities. It is simply running your existing legacy app on cloud infrastructure. Most rehosting can be done using automated tools, however a manual migration can be useful to learn how to apply the workload in the new cloud environment. Additionally, applications are often easier to optimize or re-architect once they are already running in the cloud. This party because of people, talent, and process and also partly because migration of the application, data, and traffic is already done (the hard part)."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#replatforming",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#replatforming",
    "title": "The 6 Rs of Cloud Migration",
    "section": "2. Replatforming",
    "text": "2. Replatforming\n\nAKA “lift-tinker-and-shift”\nReplatforming conserves the core architecture of an application, but leverages some cloud capabilities. An example might be replacing a database instance in your data center with a database-as-a-service platform that handles some of the operational load. Another example might be replacing queue or pub/sub service with AWS SQS."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#repurchasing",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#repurchasing",
    "title": "The 6 Rs of Cloud Migration",
    "section": "3. Repurchasing",
    "text": "3. Repurchasing\n\nAKA “pay someone else to do it better, for less”\nThis strategy is when companies move to a SaaS platform. Common examples are moving your CMR to Salesforce or an HR system to Workday."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#refactoring-re-architecting",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#refactoring-re-architecting",
    "title": "The 6 Rs of Cloud Migration",
    "section": "4. Refactoring / Re-architecting",
    "text": "4. Refactoring / Re-architecting\n\nAKA “harder, better, faster, stronger”\nThis strategy is a full blown re-imagination of the applications core architecture. It’s usually driven by business needs for new features, scale, or performance that are not feasible on-premise. It can also be part of a larger strategic initiative like shifting towards portable workloads (containers), container management platforms such as Kubernetes (I like to think of them as abstracted-compute platforms), and multi-cloud or multi-data center postures."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#retire",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#retire",
    "title": "The 6 Rs of Cloud Migration",
    "section": "5. Retire",
    "text": "5. Retire\n\nAKA “goodbye and farewell”\nSometimes it’s the better business decision to shut down an application entirely. The resources (people, money, compute) freed up from this can be put to better use."
  },
  {
    "objectID": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#retain",
    "href": "posts/2019-04-17-the-6-Rs-of-migration/2019-04-17-the-6-Rs-of-migration.html#retain",
    "title": "The 6 Rs of Cloud Migration",
    "section": "6. Retain",
    "text": "6. Retain\n\nAKA “kick the can down the road”\nRandy Pausch said “Never make a decision until you have to.” If an application does not meet the bar for taking action via one of the other five migration strategies, it will be retained."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Galen Ballew",
    "section": "",
    "text": "Galen is a leader, engineer, and lifelong learner.\nHe has 8 years of professional experience in engineering and technical leadership, with a demonstrated track record of successes in creating enterprise-scale cloud platforms, developing strategic roadmaps, and building consensus across leadership, engineering, and business stakeholders.\nHe is particularly passionate about artificial intelligence and machine learning technologies. Projects particularly interesting to Galen involve computer vision, robotics, autonomous systems, multi-modal models, and agent-based systems.\n\n \n    \n  \n    \n     email\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn"
  }
]